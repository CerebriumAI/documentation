---
title: "Training"
description: "Commands to use with fine-tuning"
---

Below are the set of commands for you to get started with fine-tuning. Please note that all fine-tuning functionality is currently done through the terminal - frontend pending.

## Deploying a training job

Deploying a training job is done using one simple command in the Cerebrium CLI.

```bash
cerebrium train <<YOUR_JOB_NAME>> <<API_KEY>> <<HUGGINGFACE_MODEL_PATH>> <<PATH_TO_YOUR_FINETUNING_DATASET>>
```

This will create a training config and upload your job and dataset to Cerebrium to start your training on an A10 instance!

## Retrieving your most recent training jobs

Keeping track of the jobIds for all your different experiments can be challenging.  
To retrieve the status and information on your 10 most recent fine-tuning jobs, you can run the following command:

```bash
cerebrium get-training-jobs <<API_KEY>>
```

Where your API_KEY is the key for the project under which your fine-tuning has been deployed. Remember if you used the cerebrium login command you don't have to paste your API Key

# Stream the logs of your fine-tuning job

To stream the logs of a specific fine-tuning job use:

```bash
cerebrium get-training-logs <<JOB_ID>>
```

# Retrieving your training results

Once your training is complete, you can download the training results using:

```bash
cerebrium download-model <<JOB_ID>> <<API_KEY>>
```

This will return a zip file which contains your **adapter** and **adapter config** which should be in the order of 10MB for your 7B parameter model due to the extreme efficiency of PEFT fine-tuning.


# Deploy your fine-tuned model

To deploy your model you can use Cortex. Below is an example that you can simply adapt in order to get deploy your model in just a few lines of code. We will be releasing 
auto-deploy functionality soon!

```python
  from transformers import AutoModelForSeq2SeqLM
+ from peft import PeftModel, PeftConfig

  peft_model_id = "path/toYourAdapter"
  config = PeftConfig.from_pretrained(peft_model_id)
  model = AutoModelForSeq2SeqLM.from_pretrained(config.base_model_name_or_path)
+ model = PeftModel.from_pretrained(model, peft_model_id)
  tokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path)

  # Now just inference your model as you normally would.
  model = model.to(device)
  model.eval()
  inputs = tokenizer("Insert your prompt matching the training template here:", return_tensors="pt")

  with torch.no_grad():
    outputs = model.generate(input_ids=inputs["input_ids"].to("cuda"), max_new_tokens=10)
    print(tokenizer.batch_decode(outputs.detach().cpu().numpy(), skip_special_tokens=True)[0])


```

For more information, see
[Using Adapter Transformers at Hugging Face](https://huggingface.co/docs/hub/adapter-transformers#exploring-adaptertransformers-in-the-hub)
