---
title: "Langchain Q&A on a YouTube Video"
description: "To deploy a Q&A application around a YouTube video"
---

In this tutorial, we'll recreate a question-answering bot based on YouTube video content, inspired by [this application](https://colab.research.google.com/drive/1sKSTjt9cPstl_WMZ86JsgEqFG-aSAwkn?usp=sharing) by @m_morzywolek.

To see the final implementation, you can view it [here](https://github.com/CerebriumAI/examples/tree/master/4-integrations/1-lanchain-QA)

## Basic Setup

Developing models with Cerebrium is similar to developing on a virtual machine or Google Colab, making conversion straightforward. Make sure you have the Cerebrium package installed and are logged in. If not, check our docs [here](https://docs.cerebrium.ai/cerebrium/getting-started/installation).

First, create your project:

```
cerebrium init 1-langchain-QA
```

Add these Python packages to the `[deps.pip]` section of your `cerebrium.toml` file:

```toml
[deps.pip]
pytube = "latest" # For audio downloading
langchain = "latest"
faiss-gpu = "latest"
ffmpeg = "latest"
openai-whisper = "latest"
transformers = ">=4.35.0"
sentence_transformers = ">=2.2.0"
```

Whisper requires ffmpeg and other Linux packages. Add them to the `[deps]` section:

```toml
[deps]
apt = [ "ffmpeg", "libopenblas-base", "libomp-dev"]
```

Create a `main.py` file for our Python code. This simple implementation accepts a YouTube video link and a question, then returns an answer with the corresponding time segment. First, let's define our request object:

```python
from pydantic import BaseModel

class Item(BaseModel):
    url: str
    question: str
```

We use Pydantic for data validation. Both `url` and `question` parameters are required - missing either will trigger an automatic error message.

## Video Transcription

We use OpenAI's Whisper model to convert video audio to text, splitting it into phrase segments with timestamps to track answer sources.

```python
import pytube
from datetime import datetime
import whisper

model = whisper.load_model("small")

def store_segments(segments):
    texts = []
    start_times = []

    for segment in segments:
        text = segment["text"]
        start = segment["start"]

        # Convert the starting time to a datetime object
        start_datetime = datetime.fromtimestamp(start)

        # Format the starting time as a string in the format "00:00:00"
        formatted_start_time = start_datetime.strftime("%H:%M:%S")

        texts.append("".join(text))
        start_times.append(formatted_start_time)

    return texts, start_times


def predict(url, question, run_id):
    item = Item(url=url, question=question)

    video = pytube.YouTube(item.url)
    video.streams.get_highest_resolution().filesize
    audio = video.streams.get_audio_only()
    fn = audio.download(output_path="/models/content/", filename= f"{video.title}.mp4")

    transcription = model.transcribe(f"/models/content/{video.title}.mp4")
    res = transcription["segments"]

    texts, start_times = store_segments(res)

```

## LangChain Implementation

We implement [LangChain](https://python.langchain.com/en/latest/index.html) to store video segments in a vector store and use a locally hosted LLM on Cerebrium to generate answers.

```python
from langchain.embeddings.openai import OpenAIEmbeddings
from langchain.text_splitter import CharacterTextSplitter
from langchain.vectorstores.faiss import FAISS
from langchain.chains import VectorDBQAWithSourcesChain
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.llms import CerebriumAI
import openai
import faiss

sentenceTransformer = SentenceTransformer("sentence-transformers/all-MiniLM-L6-v2")
os.environ["CEREBRIUMAI_API_KEY"] = "<YOUR_JWT_TOKEN>"

def create_embeddings(texts, start_times):
    text_splitter = CharacterTextSplitter(chunk_size=1500, separator="\n")
    docs = []
    metadatas = []
    for i, d in enumerate(texts):
        splits = text_splitter.split_text(d)
        docs.extend(splits)
        metadatas.extend([{"source": start_times[i]}] * len(splits))
    return metadatas, docs

    # Add to predict function after store_segments call
    metadatas, docs = create_embeddings(texts, start_times)
    embeddings = HuggingFaceEmbeddings()
    store = FAISS.from_texts(docs, embeddings, metadatas=metadatas)
    faiss.write_index(store.index, "docs.index")
    llm = CerebriumAI(
        endpoint_url="https://run.cerebrium.ai/flan-t5-xl-webhook/predict"
    )
    chain = VectorDBQAWithSourcesChain.from_llm(llm=llm, vectorstore=store)

    result = chain({"question": item.question})

    return {"result": result}

```

We chunk text segments and store them in a FAISS vector store, using a Hugging Face open-source model for embeddings. Running Whisper and the embeddings model on the same machine saves two network round trips, reducing latency by over 600ms.

The code integrates LangChain with a Cerebrium-deployed endpoint for question answering.

## Deploy

Configure your compute and environment settings in `cerebrium.toml`:

```toml

[build]
predict_data = "{\"prompt\": \"Here is some example predict data for your cerebrium.toml which will be used to test your predict function on build.\"}"
force_rebuild = false
disable_animation = false
log_level = "INFO"
disable_confirmation = false

[deployment]
name = "langchain-qa"
python_version = "3.10"
include = ["./*", "main.py"]
exclude = ["./.*", "./__*"]

[hardware]
gpu = "AMPERE_A5000"
cpu = 2
memory = 16.0
gpu_count = 1

[scaling]
min_replicas = 0
cooldown = 60

[deps.apt]
ffmpeg = "latest"
"libopenblas-base" = "latest"
"libomp-dev" = "latest"

[deps.pip]
pytube = "latest" # For audio downloading
langchain = "latest"
faiss-gpu = "latest"
ffmpeg = "latest"
openai-whisper = "latest"
transformers = ">=4.35.0"
sentence_transformers = ">=2.2.0"

[deps.conda]

```

Deploy the model using this command:

```bash
cerebrium deploy
```

After deployment, make this request:

```curl
curl --location --request POST 'https://api.aws.us-east-1.cerebrium.ai/v4/p-xxxxxx/1-langchain-QA/predict' \
--header 'Authorization: Bearer <YOUR TOKEN HERE>' \
--header 'Content-Type: application/json' \
--data-raw '{
    "url": "https://www.youtube.com/watch?v=UF8uR6Z6KLc&ab_channel=Stanford",
    "question": "How old was Steve Jobs when started Apple?"
}'
```

The endpoint returns results in this format:

```json
{
  "run_id": "8959bfaa-f6c1-4445-980c-1ab469a4b878",
  "message": "success",
  "result": {
    "result": {
      "question": "How old was Steve Jobs when started Apple?",
      "answer": "20",
      "sources": ""
    }
  },
  "run_time_ms": 72109.55119132996
}
```
