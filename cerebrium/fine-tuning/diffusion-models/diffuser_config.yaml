%YAML 1.2
---

#############################################
#  Mandatory Parameters
#############################################
training_type: "diffuser" # Type of training to run. Either "diffuser" or "transformer".
name: "test-config-file" # Name of the experiment.
api_key: "YOUR PRIVATE CEREBRIUM API KEY" # Your Cerebrium API key.
hf_model_path: "runwayml/stable-diffusion-v1-5"
train_prompt: "Photo of a tsdf dog." 
log_level: "INFO" # log_level level for logging. Can be "DEBUG", "INFO", "WARNING", "ERROR".
train_image_dir: data/training_class_images/ # Directory of training images.


#############################################
#  Optional Parameters
#############################################
# Diffuser params
revision: "main" # Revision of the diffuser model to use. 
validation_prompt: ~ # an optional validation prompt to use. If ~, will use the training prompt.
custom_tokenizer: "" # custom tokenizer from AutoTokenizer if required.

# Dataset params
prior_class_image_dir: ~ # or "path/to/your/prior_class_images". Optional directory of images to use if you would like to train prior class images as well.
prior_class_prompt: "Photo of a dog."

# Training params
training_args:
  # General training params
  num_validation_images: 4 # Number of images to generate in validation.
  learning_rate: 1e-4
  num_train_epochs: 10
  seed: ~
  resolution: 512 # Resolution to train images at.
  center_crop: False # Whether to center crop images to resolution.
  train_batch_size: 2
  num_prior_class_images: 10
  prior_class_generation_batch_size: 2
  prior_loss_weight: 1.0 # Weight of prior loss in the total loss.
  max_train_steps: ~ # maximum training steps which overrides number of training epochs
  validation_epochs: 5 # number of epochs before running validation and checkpointing

  
  # Training loop params
  gradient_accumulation_steps: 1
  learning_rate: 0.0005
  lr_scheduler: "constant"
  lr_warmup_steps: 5
  lr_num_cycles: 1
  lr_power: 1.0
  allow_tf32: False
  max_grad_norm: 1.0
  mixed_precision: "fp16"
  prior_generation_precision: ~
  scale_lr: False 
  use_8bit_adam: True
  use_xformers: True # Whether to use xformers memory efficient attention or not.
