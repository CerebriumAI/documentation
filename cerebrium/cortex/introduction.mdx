---
title: "Introduction"
description: "Deploy any Python code to serverless CPUs/GPUs giving you fine-grain control"
---

Many of our more hardcore users wanted finer-grain control over their code and package dependencies, which is why we opened up our underlying architecture to allow engineers and data scientists to deploy
any custom Python code.

We have been focusing hard on the developer experience that if your code runs locally, you can deploy it to serverless CPUs\/GPUs using one line.
Initial deploys take about a minute (installing packages etc.) however, thereafter every change is \< 20 seconds, and of course, inference requests have a cold start \<1 second!

<Note>
  Model deployment and startup time is proportional to the model size (including
  dependencies). For lightning-fast deployments, start-ups, and scale-ups keep
  your model as small as possible!
</Note>

You can get started with our simple tutorial below:

Below is a brief outline of its setup

### Components

Currently, our implementation has four components:

- **main.py** - This is where your python code lives. This is mandatory to include.
- **requirements.txt** - This is where you define your Python packages. Deployment will be quicker if you specify specific versions. This is optional to include.
- **pkglist.txt** - This is where you can define Linux packages you would like to install. We run the apt-install command for items here. This is optional to include.
- **conda_pkglist.txt** - This is where you can define Conda packages you would like to install if you prefer using it for some libraries over pip. You can use both conda and pip in conjunction. This is optional to include.

However, you can contain images and other Python files in the same directory, and they will get packaged and uploaded when you deploy.

Every main.py you deploy needs the following mandatory layout:

```bash
from pydantic import BaseModel


class Item(BaseModel):
    parameter: value


def predict(item, run_id, logger):
    item = Item(**item)

    # Do something with parameters from item

    return {"key": "value}
```

The Item class is where you define the parameters your model receives as well as their type. Item needs to inherit from BaseModel.
You need to define a function with the name **predict** which receives 3 params: item, run_id and logger.

Both the **requirements.txt** and **pkglist.txt** follow standard layouts in that each package should be on a new line.

### Deploy model

To deploy a model, download our pip package if you haven't already:

```bash
pip install --upgrade cerebrium
```

Then navigate to where your model code (specifically your `main.py`) is located and run the following command:

```bash
cerebrium deploy <MODEL_NAME> <PRIVATE_API_KEY> --hardware=<HARDWARE>
```

For the parameters above:

- **\<MODEL_NAME\>**: Give your model a name for you to recognize on your Cerebrium dashboard
- **\<PRIVATE_API_KEY\>**: This is the private API key you can find on your Cerebrium dashboard
- **\<HARDWARE\>**: This is either CPU, GPU or A10

Additionally, we provide two flags to further control which files you add to your deployment. They are:

- **--include**: Regex string of files or folders to include. eg. `--include "./modelFiles/, my-awesome-file.txt"`
- **--exclude**: The files or folders to exclude. eg. `--exclude ".venv, ./private.py, ./folderToExclude"`
These flags can be repeated to add patterns individually or can receive a string array as shown ABOVE.

Now, with just a few lines of code, your custom Python code will be deployed to serverless CPUs/GPUs without setting up any infrastructure! As simple as that!  
After the first deployment, every subsequent deployment should be less than 10s, keeping the development lifecycle as quick as possible.

Once you deploy a model, navigate back to the dashboard and click on the name of the model you just
deployed. You will see the usual overview statistics of your model, but most importantly, you will see two tabs titled <b>builds</b> and <b>runs</b>.

- <b>Builds</b>: This is where you can see the logs regarding the creation of
  your environment and the code specified in the <b>Init</b> function. You will
  see logs only on every deploy.
- <b>Runs</b>: This is where you will see logs concerning every API call to your
  model endpoint. You can therefore debug every run based on input parameters
  and the model output.


We recommend you check out the [advanced functionality](/cerebrium/cortex/advanced-functionality) section of Cortex as well as work through the [examples](/coretex/examples) to see how deployments were done for various different sample applications.
