---
title: Testing with Hot-reload
description: Use the `cerebrium serve` command to rapidly iterate on your code
---

When you are developing a `cortex` deployment on **Cerebrium**, waiting for a build to complete can be time-consuming. To speed up your development process, you can use the `cerebrium serve` command to rapidly iterate on your deployment.

This allows you to run your deployment on a dedicated server, and see the results of your changes in a few seconds.

<Note>
  This feature is currently in beta and is available to all users. As such, you
  may encounter bugs or limitations. We are actively working on improving the
  experience and adding more features. If you have any feedback or suggestions,
  we'd love to hear from you on discord or slack!
</Note>

## Usage

To start a served instance, first navigate to the root folder of your **cortex deployment**.

Then, simply run the following command in your terminal:

```bash
cerebrium serve start
```

After running the `cerebrium serve start` command, you will see output in your terminal indicating that the server is starting up. This output will include a URL that you can use to access your deployment locally.

Here are some additional commands that can be useful when working with `cerebrium serve`:

To stop a served instance, you can simply press `Ctrl+C` in the terminal where the server is running. This will gracefully shut down the server and terminate the served instance.

### Testing inference on the served instance

#### Using the local REST server

When you run `cerebrium serve start`, the CLI will print a local REST server address that you can use to send requests to your deployment.

To use the local server to test inference, you can send a **POST** request to: `localhost:8000/predict`
and the body of the request should be a json object with the input data for your deployment.

Here is an example curl command:

```bash
curl -X POST http://localhost:8000/predict -H "Content-Type: application/json" --data '{"prompt": "this is my input data"}'
```

Since we are using a local api server, you don't need to worry about providing an api key or any other authentication details.
Otherwise, you can interact with this endpoint as you would with a normal Cerebrium endpoint.
Your served instance will be running on port 8000 by default, however, you can change this by using the `--port` flag when you start a served instance

Additionally, if you have more than one serve instance running, you can specify the id of the serve instance you want to send the request by using the following pattern: `localhost:8000/{serve_id}/predict` and we'll make sure the request is sent to the correct instance.

## How it works

When you run `cerebrium serve start`, the following happens:

1. The `cerebrium` CLI uploads your deployment to a dedicated instance(s).
2. The server builds your deployment in the same way as `cerebrium deploy`.
3. The server starts your deployment and waits for you to send in requests.
4. If you make changes to your main.py or other code in your deployment, the server reloads your deployment and applies your changes without rebuilding the entire deployment.
5. When you're done, you can stop the server by pressing `Ctrl+C` in the same terminal where you started the server.

**Note:**

- Build process (packages, apt, etc.) changes require a full restart of the served instance
- Serve sessions will automatically terminate after 10 minutes of inactivity
- Serve sessions are billable for as long as a session is running
