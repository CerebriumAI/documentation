---
title: "Whisper"
description: "Whisper is a language generation model that receives audio data and performs transcription and or translation, then outputs a text response."
api: "https://api.cerebrium.ai"
---

It can process audio of any length and takes about 1 minute to process an hour of audio. We also return the timestamps of the transcription if you would like to create subtitles for the audio etc.
We currently have the following whisper models available below however if you would like any others contact support and we can quickly add it for you. In order to deploy it you can use the identifier below:

- Whisper Medium: `whisper-medium`

Once you've deployed a Whisper model, you can supply the enpoint with a base-64 encoded audio file.  Here's an example of how to call the deployed endpoint:

#### Request Parameters

<RequestExample>
```bash Request
  curl --location --request POST 'https://inference.cerebrium.ai/runs/<YOUR_ENDPOINT>' \
      --header 'Authorization: <API_KEY>' \
      --header 'Content-Type: application/json' \
      --data-raw '{
        "audio": "<BASE_64_STRING>"
        "webhook_endpoint": "<WEBHOOK_ENDPOINT>"
        }'
```
</RequestExample>

<ParamField header="Authorization" type="string" required>
  This is the Cerebrium API key used to authenticate your request. You can get it from your Cerebrium dashboard.
</ParamField>
<ParamField body="audio" type="string" required>
   A base64 encoded string of the audio file you would like to transcribe/translate.
</ParamField>

<ResponseExample>

```json Response
{
    "text":" "<TEXT>",
    "segments": []
}
```
</ResponseExample>

#### Response Parameters

<ResponseField name="text" type="string" required>
  The text that has been transcribed or translated
</ResponseField>
<ResponseField name="segments" type="string" required>
  Detailed information about the transcription/translation.
</ResponseField>