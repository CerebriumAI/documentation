---
title: Training Falcon Models
description: "Guide to fine-tuning Falcon models using Cerebrium"
---

## Introduction

<!-- TODO: An intro to falcon and why someone might want to use it. -->

`Falcon` is a family of models released by the Technology Innovation Institute (TII) of the UAE.
At the time of release, it topped the Huggingface leaderboards while also being one of the fastest LLMs available.
Additionally, the TII team have released the model under an Apache 2.0 license, making it available for commercial use.

The `Falcon` family of models are available in 2 sizes:

- `Falcon 7B` - 7 billion parameters
- `Falcon 40B` - 40 billion parameters

These models come in two variants:

- The raw, pretrained models `Falcon-7B/40B`
- The instruct trained models `Falcon-7B/40B-instruct`
  These have been further trained for instruct/chat capabilities.

We provide full support for the finetuning of the Falcon 7B model.  
The finetuning of the Falcon 40B model is currently in testing and will be released to the public soon.

## Getting Started

### Creating a Project

To create a project, you will need to initialise a training configuration file and curate a [dataset](/cerebrium/fine-tuning/language-models/dataset).

<!-- 
We provide an example configuration file for the Falcon 7B model which can be used as a starting point for your own configuration file. Just run the following command:
```bash
cerebrium init-trainer --trainer-type falcon --trainer-name my-falcon-trainer
``` -->

<!-- TODO this init needs to be made before uncommenting -->

### Adjusting your config file

Some important parameters to take note of in your config file are the following:

| Parameter         | Description                                                                                | Required Value        |
| ----------------- | ------------------------------------------------------------------------------------------ | --------------------- |
| target_modules    | The modules to apply PEFT to. For Falcon, this parameter is different to Llama, GPT2, etc. | `["query_key_value"]` |
| trust_remote_code | Whether to trust the remote code. Required to setup the falcon model                       | `true`                |
| load_in_8bit      | Whether to load the model in 8bit. Set to `True` for Falcon-7B.                            | `true`                |

<!-- | load_in_4bit      | Whether to load the model in 4bit. Set to `True` for Falcon-40B.                           | `true`                | -->

<!-- TODO:
- which parameters to adjust and when
- epochs
- learning rate
- prompt template
  - Should we include how to add stop tokens etc? -->

## Train your model
Training your model follows the same process as training any other model on Cerebrium, you'll need to run the same `cerebrium train` command.

```bash
cerebrium train <<YOUR CONFIG FILE>> 
```

See the [training](/cerebrium/fine-tuning/language-models/training) page for more information on training your model.


## Evaluate your model
The output of training a Falcon model on Cerebrium is an adapter file. Once you have downloaded your adapter file (see the instructions [here](/cerebrium/fine-tuning/language-models/training)), it can be used to load the model into your own code and run inference on it.

To load the model into your Cortex deployment, you can use the following code snippet:

```


```python
# Your normal imports in Cortex:
import base64
from typing import Optional

# ADD THE FOLLOWING TO YOUR IMPORTS
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel, PeftConfig
import torch

# ADD THE FOLLOWING TO YOUR MODEL SETUP IN YOUR MAIN.PY
peft_model_id = "results/" # Path to your results/ folder which contains "adapter_model.bin" and "adapter_config.json" 
config = PeftConfig.from_pretrained(peft_model_id)
model = AutoModelForCausalLM.from_pretrained(config.base_model_name_or_path,
                                             low_cpu_mem_usage=True, # You may require this file to avoid memory issues
                                             load_in_8bit=True,
                                             trust_remote_code=True,
                                             device_map="auto")
model = PeftModel.from_pretrained(model, peft_model_id, trust_remote_code=True) # Add the adapter to the model
tokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path)

model = model.to("cuda")
```
Your model is now ready to run inference on.

To have control of the number of new tokens generated, (as well as other generation parameters) you can add a `max_new_tokens` parameter to your `Item` class in your `main.py` file.
```python
class Item(BaseModel):
    prompt: str
    max_new_tokens: Optional[int] = 250 # Add this line to your Item class
```

You can then add the following code to your `predict` function to run inference on your model:
```python
# ADD THE FOLLOWING TO YOUR PREDICT FUNCTION IN YOUR MAIN.PY
def predict(item, run_id, logger):
    item = Item(**item)
     # REPLACE THIS WITH YOUR TEMPLATE USED FOR TRAINING
    template =  "### Instruction:\n{instruction}\n\n### Response:\n" 

    question = template.format(instruction=prompt) # Place the prompt into the template
    inputs = tokenizer(question, return_tensors="pt")

    with torch.no_grad(): # Run inference on the model
      outputs = model.generate(input_ids=inputs["input_ids"].to("cuda"), max_new_tokens=item.max_new_tokens)
      result = tokenizer.batch_decode(outputs.detach().cpu().numpy(), skip_special_tokens=True)[0]
    return result
```

## Deploy your model

Your model can now be deployed using Cerebrium's Cortex. Just ensure that your adapter files are in the same directory as your `main.py` file and run `cerebrium deploy` as you would for any other model.

# Falcon 7B vs Falcon 40B

<!-- TODO this needs to become a comparison table -->

## Falcon 40B config:

```yaml
training_type: transformer

name: your-falcon-40b-name
api_key: YOUR API KEY HERE
hf_model_path: tiiuae/falcon-40b
model_type: AutoModelForCausalLM
dataset_path: Your/Dataset/path/dataset.json
custom_tokenizer: ""
seed: 42
log_level: INFO
training_args:
  logging_steps: 10
  per_device_train_batch_size: 2 # TODO These have not been optimised yet. Will depend on hardware
  per_device_eval_batch_size: 2
  warmup_steps: 0
  gradient_accumulation_steps: 4
  num_train_epochs: 3 # Just for testing. Increase to 30 or 50 for better results
  learning_rate: 2.0E-4
  group_by_length: false
  fp16: true
  max_grad_norm: 0.3
  lr_scheduler_type: constant
peft_lora_args:
  r: 32
  lora_alpha: 16
  lora_dropout: 0.05
  target_modules: ["query_key_value"]
  bias: none
  task_type: CAUSAL_LM
base_model_args:
  # load_in_8bit: true
  load_in_4bit: true
  device_map: auto
  trust_remote_code: true
```

## Falcon 7B config:

```yaml
%YAML 1.2
---
training_type: "transformer" # Type of training to run. Either "diffuser" or "transformer".

name: your-falcon-7b-name # Name of the experiment.
api_key: Your API KEY HERE # Your Cerebrium API key.

# Model params:
hf_model_path: "tiiuae/falcon-7b"
model_type: "AutoModelForCausalLM"
dataset_path: /path/to/your/dataset.json # path to your local JSON dataset.
custom_tokenizer: "" # custom tokenizer from AutoTokenizer if required.
seed: 42 # random seed for reproducibility.
log_level: "INFO" # log_level level for logging.

# Training params:
training_args:
  logging_steps: 10
  per_device_train_batch_size: 2
  per_device_eval_batch_size: 2
  warmup_steps: 0
  gradient_accumulation_steps: 4
  num_train_epochs: 3
  learning_rate: 2.0e-4
  group_by_length: False
  fp16: True
  max_grad_norm: 0.3
  # max_steps: 1000
  lr_scheduler_type: "constant"

base_model_args: # args for loading in the base model.
  load_in_8bit: True
  device_map: "auto"
  trust_remote_code: True

peft_lora_args: # peft lora args.
  r: 32
  lora_alpha: 16
  lora_dropout: 0.05
  target_modules: [query_key_value]
  bias: "none"
  task_type: "CAUSAL_LM"
```
