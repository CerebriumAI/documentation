---
title: "XGBoost"
description: "This example provides the steps to deploy a simple XGBoost model (from scratch!) using Cerebrium."
---

By the end of this guide, you'll have an API endpoint that can handle any scale
of traffic by running inference on serverless CPUs/GPUs.

## Project set up

Before building you need to set up a Cerebrium account. This is as simple as
starting a new Project in Cerebrium and copying the API key. This will be used
to authenticate all calls for this project.

### Create a project

1. Go to [dashboard.cerebrium.ai](https://dashboard.cerebrium.ai)
2. Sign up or Login
3. Navigate to the API Keys page
4. You should see an API key with the source "Cerebrium". Click the eye icon to
   display it. It will be in the format: c_api_key-xxx

![API Key](/images/quickstarts/onnx-1.png)

### Develop model

Now navigate to where your model code is stored. This could be in a notebook or
in a plain `.py` file.

To start, you should install the Cerebrium framework by running the following
command in your notebook or terminal

```bash
pip install --upgrade cerebrium
```

Copy and paste our code below. This creates a simple XGBoost classifier on the Iris dataset. This code could be replaced by any XGBoost model. Make sure you have
the required libraries installed.

```bash
from sklearn.datasets import load_iris
from xgboost import XGBClassifier

iris = load_iris()
X, y = iris.data, iris.target

xgb = XGBClassifier()
xgb.fit(X, y)

# Save to XGB JSON
xgb.save_model("iris.json")
```

In the last line of code, there are two ways you can save the model. This is all
you need to deploy your model to Cerebrium! You can then import the `deploy()`
function from the Cerebrium framework. I used the CloudPickle function to save
my model below.

```bash
from cerebrium import deploy, model_type

endpoint = deploy((model_type.XGBOOST_CLASSIFIER, "iris.json"), "xgb-test-model" , "<API_KEY>")
```

![Deployed Model](/images/quickstarts/xgboost-1.png)

<Note>
  Your result format will change for the `_classifier` model types. The
  `xgb_classifier` will return a `result` object containing the probability
  distribution for the predicted output classes, rather than the argmax of the
  distribution. This is to allow you flexibility in how you want to handle the
  output of your model for classification. For example, you may want to return
  the top 3 predictions for your model, or you may want to return the top 3
  predictions with a minimum probability threshold. This is up to you.
</Note>

Your model is now deployed and ready for inference all in under **10 seconds**!
Navigate to the dashboard and on the Models page, you will see your model.

You can run inference using `curl`

```bash
curl --location --request POST '<ENDPOINT>' \
--header 'Authorization: <API_KEY>' \
--header 'Content-Type: application/json' \
--data-raw '[[5.1, 3.5, 1.4, 0.2]]'
```

The response will be:

![XGB Postman Response](/images/quickstarts/xgboost-2.png)

Navigate back to the dashboard and click on the name of the model you just
deployed. You will see an API call was made and the inference time. From your dashboard, you can monitor your model, roll back to previous versions and see traffic.

![XGB Monitoring](/images/quickstarts/xgboost-3.png)

With one line of code, your model was deployed in seconds with automatic
versioning, monitoring and the ability to scale based on traffic spikes. Try
deploying your own model now or check out our other frameworks.
