---
title: Decrease Model Loading Time
description: Decrease the time it takes to load your model from storage into GPU
---

One of the biggest contributions to the startup time of your model is the time it takes to load your model from storage into GPU memory. For example, in larger models of 20B+ parameters, it can take >40s for your model to be loaded using a normal huggingface load, even with the 2GB/s transfer speeds from persistent storage.

While we've optimised the underlying hardware to load models as fast as possible, there are a few things you can do to decrease the time it takes to load your model and, therefore, your coldstart times.

# Using a serialisation and zero-copy initialisation libraries

## Tensorizer (recommended)

[Tensorizer](https://github.com/coreweave/tensorizer) is a library that allows you to load your model from storage into GPU memory in a single step.  
While initially built to fetch models from S3, it can be used to load models from file as well and so, can be used to load models from Cerebrium's persistent storage which features a near 2GB/s read speed.
In the case of large models of 20B+ parameters, we've observed a **30-50%** decrease in model loading time which further increases with larger models.  
For more information on the underlying methods, take a look at their github page [here](https://github.com/coreweave/tensorizer).

In this section below, we'll show you how to use **Tensorizer** to load your model from storage straight into GPU memory in a single step.

### Installation

Add the following to your `requirements.txt` file to install **Tensorizer** in your deployment:

```txt
tensorizer
```

### Usage

To use **Tensorizer**, you need to first serialise your model and save it to your persistent-storage.

```python
from tensorizer import TensorSerializer
def serialise_model(model, save_path):
    """Serialise the model and save the weights to the save_path"""
    try:
        serializer = TensorSerializer(save_path)
        start = time.time()
        serializer.write_module(model)
        end = time.time()
        print((f"Serialising model took {end - start} seconds"),  file=sys.stderr)
        serializer.close()
        return True
    except Exception as e:
        print("Serialisation failed with error: ", e,  file=sys.stderr)
        return False
```

This will convert your model to a protocol buffer serialised format that is optimised for faster transfer speeds and fast loading into GPU memory.

Then, the next time your deployment starts, you can load your serialised model from storage into GPU memory in a single step.
You would do this as follows:

```python

from tensorizer import TensorDeserializer
from tensorizer.utils import no_init_or_tensor
def deserialise_saved_model(model_path, model_id, plaid=True):
    """Deserialise the model from the model_path and load into GPU memory"""

    # create a config object that we can use to init an empty model
    config = AutoConfig.from_pretrained(model_id)

    # Init an empty model without loading weights into gpu. We'll load later.
    print(("Initialising empty model"),  file=sys.stderr)
    start = time.time()
    with no_init_or_tensor():
        # Load your model here using whatever class you need to initialise an empty model from a config.
        model = AutoModelForCausalLM.from_config(config)
    end_init = time.time() - start

    # Create the deserialiser object
    #   Note: plaid_mode is a flag that does a much faster deserialisation but isn't safe for training.
    #    -> only use it for inference.
    deserializer = TensorDeserializer(model_path, plaid_mode=True)

    # Deserialise the model straight into GPU (zero-copy)
    print(("Loading model"),  file=sys.stderr)
    start = time.time()
    deserializer.load_into_module(model)
    end = time.time()
    deserializer.close()

    # Report on the timings.
    print(f"Initialising empty model took {end_init} seconds",  file=sys.stderr)
    print((f"\nDeserialising model took {end - start} seconds\n"),  file=sys.stderr)

    return model
```

Placing this all together in a script that you could use to load your model would look like this.

```python
import time
from huggingface_hub import hf_api
import torch
import os
import sys
from transformers import AutoModelForCausalLM, AutoConfig


# Setup the paths to the model cache
APP_NAME = os.environ.get("APP_NAME", "default") # this gets the name of your deployment from the environment variables
private_model_path = f"/persistent-storage/.cache/tensorised-models/{APP_NAME}/{{org}}/{{model_name}}.tensors"

def fast_load(model_id, load_weights_func, faster=False):
    """A function that loads a model from the cache if it exists, or serialises it if it doesn't
    Args:
        model_id (str): The model id in the form of org/model_name
        load_weights_func (function): Some function where you load your model, send it to GPU and prep it for inference
        faster (bool, optional): Whether to use the faster method of loading the. Defaults to False."""
    org, model_name = model_id.split("/")
    model_path = private_model_path.format(org=org, model_name=model_name)

    # Check if the model exists in the cache
    if os.path.isfile(model_path):
        print((f"Deserialising model from {model_path}"),  file=sys.stderr) # printing to sterr prints the output immediately. Otherwise it will be buffered.
        model = deserialise_saved_model(model_path, model_id, plaid=faster)
    else:
        # some function where you load your model, send it to GPU and prep it for inference
        model = load_weights_func()
        print((f"Serialising model to {model_path}"),  file=sys.stderr)
        serialise_model(model, model_path)
    return model


def deserialise_saved_model(model_path, model_id, plaid=True):
    """Deserialise the model from the model_path and load into GPU memory"""
    from tensorizer import TensorDeserializer
    from tensorizer.utils import no_init_or_tensor


    # create a config object that we can use to init an empty model
    config = AutoConfig.from_pretrained(model_id)

    # Init an empty model without loading weights into gpu. We'll load later.
    print(("Initialising empty model"),  file=sys.stderr)
    start = time.time()
    with no_init_or_tensor():
        # Load your model here using whatever class you need to initialise an empty model from a config.
        model = AutoModelForCausalLM.from_config(config)
    end_init = time.time() - start

    # Create the deserialiser object
    #   Note: plaid_mode is a flag that does a much faster deserialisation but isn't safe for training.
    #    -> only use it for inference.
    deserializer = TensorDeserializer(model_path, plaid_mode=True)

    # Deserialise the model straight into GPU (zero-copy)
    print(("Loading model"),  file=sys.stderr)
    start = time.time()
    deserializer.load_into_module(model)
    end = time.time()
    deserializer.close()

    # Report on the timings.
    print(f"Initialising empty model took {end_init} seconds",  file=sys.stderr)
    print((f"\nDeserialising model took {end - start} seconds\n"),  file=sys.stderr)

    return model


def serialise_model(model, model_path):
    """Serialise the model and saving the weights to the model_path"""
    from tensorizer import TensorSerializer
    try:
        serializer = TensorSerializer(model_path)
        start = time.time()
        serializer.write_module(model)
        end = time.time()
        print((f"Serialising model took {end - start} seconds"),  file=sys.stderr)
        serializer.close()
        return True
    except Exception as e:
        print("Serialisation failed with error: ", e,  file=sys.stderr)
        return False
```
