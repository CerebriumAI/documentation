---
title: "Training Diffusion Models"
description: "Guide to fine-tuning a diffusion model on Cerebrium"
---

- [Running a diffuser fine-tuning](#running-a-diffuser-fine-tuning)
  - [Starting a job with the CLI](#starting-a-job-with-the-cli)
    - [Monitoring](#monitoring)
- [Config files](#config-files)
  - [Setting up a config file](#setting-up-a-config-file)
    - [Optional parameters](#optional-parameters)
      - [Training parameters](#training-parameters)
- [Using your Fine-tuned Diffuser](#using-your-fine-tuned-diffuser)

# Running a diffuser fine-tuning

Diffusion models are incredible at generating images from art, designs, logos and beyond. Fine-tuning stable diffusion models allows you to create your very own personal model which generates images of your object or in your style.  
Using Cerebrium's trainer, you can fine-tune your model with one command using your very own data without having worry about any of the hardware or implementation details!

Our diffusion fine-tuner leverages parameter efficient finetuning (PEFT) and low rank adaptors (LORA) to only train a small subset of the diffusion model. This drastically improves training time while focusing on the parts of the model that make the biggest difference to your results, saving you time while yielding results comparable to full network finetuning.

While in the beta phase, the following models are supported for fine-tuning on Cerebrium:

| Model Name                           | Huggingface Path               |
| ------------------------------------ | ------------------------------ |
| Stable Diffusion V1.5                | runwayml/stable-diffusion-v1-5 |
| Stable Diffusion V2.1 (coming soon!) | stabilityai/stable-diffusion-2 |

We're actively working on adding more models based on demand so please let the team know if there's a model you would like us to add support for.

## Starting a job with the CLI

Starting a job on cerebrium requires four things:

- A name for you to use to identify the training job.
- Your API key.
- A config file or json string. See [this section](#config-files) for more info.
- Your dataset of training images along with the corresponding prompt.

Once you have these, you can start a fine-tuning job using the following command:

```bash
cerebrium train --name <<Name for your training>> --training-tupe "diffuser" --api-key <<Your api key if you haven't logged in>> --config-file <<Path to your config file>>
```

Your `config-file` or `config-string` could alternatively provide all the other parameters.

### Monitoring

Once your training job has been deployed, you will recieve a **job-id**. This can be used to access the job status as well as retrieve the logs.

Checking the training status:

```bash
cerebrium get-training-jobs --last-n <optional parameter to only return the status of the last n runs>
```

Retrieving the training logs can be done with:

```bash
cerebrium get-training-logs <<Your training job ID>>
```

_Note that your training logs will only be availible once the job has started running and will not be stored after it is complete._

_Coming soon: logging your training with **weights and biases** is in the final stages of development._

# Config files

In order to simplify the training experience and make deployments easier, we use yaml config files. This lets you keep track of all your training parameters in one place, giving you the flexibility to train your model with the parameters you need while still keeping the deployment process streamlined.

In this section, we introduce the parameters you may want to manipulate for your training ,however, you we'll set the defaults to what we've found works well if you'd like to leave them out!

If you would like to override the parameters in your config file during a deployment, the `--config-string` option in the command line accepts a JSON string. The parameters provided in the JSON string will override the values they may have been assigned in your config file.

For your convenience, an example of the config file is availible [here](diffuser_config.yaml):

<!-- TODO: Check the link to the file is working. -->

## Setting up a config file

Your yaml config file can be placed anywhere on your system, just point the trainer to the file.

In your yaml file, you need to include the following required parameters or parse them into the `cerebrium train` command:

| Parameter               | Suggested Value                | Description                                                                          |
| ----------------------- | ------------------------------ | ------------------------------------------------------------------------------------ |
| **Required parameters** |
| training_type           | diffuser                       | Type of training to run. Either diffuser or transformer. In this case, use diffuser. |
| name                    | test-config-file               | Name of the experiment.                                                              |
| api-key                 | Your private API key           | Your Cerebrium API key.                                                              |
| hf_model_path           | runwayml/stable-diffusion-v1-5 | ---                                                                                  |
| train_prompt            | Photo of a tsdf dog.           | ---                                                                                  |
| log_level               | INFO                           | log_level level for logging. Can be DEBUG, INFO, WARNING, ERROR.                     |
| train_image_dir         | data/training_class_images/    | Directory of training images.                                                        |

### Optional parameters

Additionally, you can include the following parameters in your config file:

| Parameter                        | Suggested Value         | Description                                                                |
| -------------------------------- | ----------------------- | -------------------------------------------------------------------------- |
| **Optional Diffuser Parameters** |
| revision                         | main                    | Revision of the diffuser model to use.                                     |
| validation_prompt                | ~                       | an optional validation prompt to use. Will use the training prompt if "~". |
| custom_tokenizer                 | ~                       | custom tokenizer from AutoTokenizer if required.                           |
| **Dataset params**               |
| prior_class_image_dir            | data/prior_class_images | Optional directory of images to use for prior class.                       |
| prior_class_prompt               | Photo of a dog.         |

#### Training parameters

The following training parameters can be included if you need to modify the training requirements. These must be child-parameters which fall under the **training_args** parameter in your config file.

| Parameter     | Sub parameter                     | Suggested Value | Description                                                                                                        |
| ------------- | --------------------------------- | --------------- | ------------------------------------------------------------------------------------------------------------------ |
| training_args |                                   |                 |                                                                                                                    |
|               | num_validation_images             | 4               | Number of images to generate at each validation step.                                                              |
|               | num_train_epochs                  | 50              | Number of epochs to train for                                                                                      |
|               | seed                              | ~               | manual seed to set for training.                                                                                   |
|               | resolution                        | 512             | Resolution to train images at.                                                                                     |
|               | center_crop                       | False           | Whether to center crop images to resolution.                                                                       |
|               | train_batch_size                  | 2               | Batch size for training. Will significantly affect memory usage so we suggest leaving at 2.                        |
|               | num_prior_class_images            | 10              | Number of images to generate using the **prior class prompt**                                                      |
|               | prior_class_generation_batch_size | 2               | Batch size for generating prior class images. We suggest leaving at 2.                                             |
|               | prior_generation_precision        | ~               | Precision used to generate prior class images if applicable.                                                       |
|               | prior_loss_weight                 | 1.0             | Weight of prior loss in the total loss.                                                                            |
|               | max_train_steps                   | ~               | maximum training steps which overrides number of training epochs                                                   |
|               | validation_epochs                 | 5               | number of epochs before running validation and checkpointing                                                       |
|               | gradient_accumulation_steps       | 1               |
|               | learning_rate                     | 0.0005          | The learning rate you would like to train with. Can be more aggressive than you would use to train a full network. |
|               | lr_scheduler                      | constant        | Learning rate schedule. Can be one of ["constant", "linear", "cosine", "polynomial"]                               |
|               | lr_warmup_steps                   | 5               | Number of learning rate warmup steps.                                                                              |
|               | lr_num_cycles                     | 1               | Number of learning rate cycles to use.                                                                             |
|               | lr_power                          | 1.0             | Power factor if using a spolynomial scheduler.                                                                     |
|               | max_grad_norm                     | 1.0             |
|               | mixed_precision                   | fp16            | If you would like to use mixed precision. Supports fp16 and bf16 else train in fp32.                               |
|               | scale_lr                          | False           | Scale the learning rate according to the gradient accumulation steps, training batch size and number of processes. |
|               | allow_tf32                        | False           | Allow matmul using tf32. Defaults to False.                                                                        |
|               | use_8bit_adam                     | True            | Use 8 bit adam for lower memory usage.                                                                             |
|               | use_xformers                      | True            | Whether to use xformers memory efficient attention or not.                                                         |

# Using your Fine-tuned Diffuser

Using your finetuning results is done as follows:

```python

from diffusers import (
    DiffusionPipeline,
    DPMSolverMultistepScheduler,
)
import torch

# Boilerplate loading of model
pipeline = DiffusionPipeline.from_pretrained(
    your_model_name, revision=your_model_revision, torch_dtype=torch.float16
)
pipeline.scheduler = DPMSolverMultistepScheduler.from_config(pipeline.scheduler.config)
pipeline = pipeline.to("cuda")


# LOAD IN YOUR TRAINING RESULTS
# load attention processors from where they are saved in your_results/checkpoints/final/attn_procs/pytorch_lora_weights.bin
pipeline.unet.load_attn_procs(os.path.join(final_output_dir, "attn_procs"))
# And that's all you need to do to load in the finetuned result!
# Now you can run your inference as you would like with the pipeline.


# some inference variables
your_prompt = "Your training prompt that you would like to use here"
num_images =  4 # number of images to generate
your_manual_seed = 42 # a manual seed if you would like repeatable results

# run inference as you normally would
generator = torch.Generator(device="cuda").manual_seed(your_manual_seed)
images = [
    pipeline(your_prompt, num_inference_steps=25, generator=generator).images[0]
    for _ in range(num_images)
]

```

<!--
# Example yaml config file
```yaml
#############################################
#  Mandatory Parameters
#############################################

#############################################
#  Optional Parameters
#############################################
# Diffuser params
# revision: "main" # Revision of the diffuser model to use.
# validation_prompt: ~ # an optional validation prompt to use. If ~, will use the training prompt.
# custom_tokenizer: "" # custom tokenizer from AutoTokenizer if required.

# # Dataset params
# train_image_dir:  data/training_class_images/ # Directory of training images.
# prior_class_image_dir: ~ # "data/prior_class_images" # Optional directory of images to use for prior class.
# prior_class_prompt: "Photo of a dog."

# # Training params
# training_args:
#   # General training params
#   num_validation_images: 4 # Number of images to generate in validation.
#   learning_rate: 1e-4
#   num_train_epochs: 10
#   seed: ~
#   resolution: 512 # Resolution to train images at.
#   center_crop: False # Whether to center crop images to resolution.
#   train_batch_size: 2
#   num_prior_class_images: 10
#   prior_class_generation_batch_size: 2
#   prior_loss_weight: 1.0 # Weight of prior loss in the total loss.
#   max_train_steps: ~ # maximum training steps which overrides number of training epochs
#   validation_epochs: 5 # number of epochs before running validation and checkpointing

#   # Training loop params
#   gradient_accumulation_steps: 1
#   learning_rate: 0.0005
#   lr_scheduler: "constant"
#   lr_warmup_steps: 5
#   lr_num_cycles: 1
#   lr_power: 1.0
#   allow_tf32: False
#   max_grad_norm: 1.0
#   mixed_precision: "fp16"
#   prior_generation_precision: ~
#   scale_lr: False
#   use_8bit_adam: True
#   use_xformers: True # Whether to use xformers memory efficient attention or not.
``` -->
