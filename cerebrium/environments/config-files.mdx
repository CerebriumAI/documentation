---
title: Using Config Files
description: Using config files to configure Cortex deployments easily and quickly
---

After you've created your Cortex project, you may find that you need more control over your deployment than the default `cerebrium deploy` command provides.
For example, you may want to specify the number of GPUs to use, the amount of memory to use or even the version of Python for your environment.
These settings and more are all configurable using config files.

Your config file is a TOML file that you can use to specify the parameters of your cortex app. This file is used to specify the deployment parameters, build parameters, hardware parameters, scaling parameters and dependencies for your deployment.

## Creating a config file

The fastest and simplest way to create a config file is to run the `cerebrium init` command.
This command creates a `cerebrium.toml` file in the project root, which you can then edit to suit your needs.
You can specify any field you wish to be prepopulated with a specific value.

```bash
cerebrium init my-project-dir --name=<your-app-name> --gpu=ADA_L4
```

## Deployment Parameters

Deployment parameters govern the persistent environment in which your app is deployed.
These parameters are specified under the `cerebrium.deployment` section of your config file.

The available deployment parameters are:

| parameter               | description                                                              | type          | default                |
| ----------------------- | ------------------------------------------------------------------------ | ------------- | ---------------------- |
| `name`                  | The name of your app                                                     | string        | my-app                 |
| `python_version`        | The Python version available for your runtime                            | float         | {interpreter_version}  |
| `include`               | Local files to include in the deployment.                                | list\[string] | \["./\*", "main.py"]   |
| `exclude`               | Local Files to exclude from the deployment.                              | list\[string] | \["./.\*"]             |
| `docker_base_image_url` | The base Docker image to build your environment from                     | string        | 'debian:bookworm-slim' |
| `shell_commands`        | A list of commands to run an app entrypoint script                       | list\[string] | []                     |
| `pre_build_commands`    | A list of commands to run before any files are copied to the environment | list\[string] | []                     |

## Hardware Parameters

The hardware parameters section is where you can define the specifications of the machine to use for your app. This allows you to tailor your app to your specific needs, optimizing for cost or performance as you see fit.
These parameters are specified under the `cerebrium.hardware` section of the config file.

The available hardware parameters are:

| parameter   | description                                           | type   | default   |
| ----------- | ----------------------------------------------------- | ------ | --------- |
| `cpu`       | The number of CPU cores to use.                       | int    | 2         |
| `memory`    | The amount of Memory to use in GB.                    | float  | 14        |
| `compute`   | The Cerebrium Identifier for your specified hardware. | string | CPU       |
| `gpu_count` | The number of GPUs.                                   | int    | 1         |
| `provider`  | The provider to run on. v4 only supports `aws`        | string | aws       |
| `region`    | The region to deploy to. v4 only supports `us-east-1` | string | us-east-1 |

### Available Hardware for `compute`

The following is the hardware available on the Cerebrium. We use a unique identifier for each GPU model:

| GPU Model                                                                | Cerebrium Identifier | VRAM |
| ------------------------------------------------------------------------ | -------------------- | ---- |
| [NVIDIA A10](https://www.nvidia.com/en-us/data-center/products/a10-gpu/) | `AMPERE_A10`         | 24GB |
| [NVIDIA L40s](https://www.nvidia.com/en-us/data-center/l40s/)            | `ADA_L40`            | 48GB |
| [NVIDIA L4](https://www.nvidia.com/en-us/data-center/l4/)                | `ADA_L4`             | 24GB |
| [NVIDIA T4](https://www.nvidia.com/en-us/data-center/tesla-t4/)          | `TURING_T4`          | 16GB |
| [AWS INFERENTIA 2](https://aws.amazon.com/machine-learning/inferentia/)  | `INF2`               | 32GB |
| [AWS TRANIUM 1](https://aws.amazon.com/machine-learning/trainium/)       | `TRN1`               | 32GB |

There are more models available through the _Enterprise Plan_. If you would like to use any of the following,
please contact us!

| GPU Model                                                          | Cerebrium Identifier | VRAM |
| ------------------------------------------------------------------ | -------------------- | ---- |
| [NVIDIA H100](https://www.nvidia.com/en-us/data-center/h100/)      | `HOPPER_H100`        | 80GB |
| [NVIDIA A100_80GB](https://www.nvidia.com/en-us/data-center/a100/) | `AMPERE_A100`        | 80GB |
| [NVIDIA A100_40GB](https://www.nvidia.com/en-us/data-center/a100/) | `AMPERE_A100_40GB`   | 40GB |

If you would like to deploy an app with no GPU requirement, you can use the `CPU` Cerebrium Identifier for
your `compute`.

For more details on hardware choice, consult the [Available Hardware page](available-hardware)

## Scaling Parameters

This section provides configuration options for deployment scaling. These parameters control the minimum and maximum number of replicas to run, as well as the cooldown period between scaling events. For example, increasing the cooldown time or setting a minimum number of replicas can enhance availability and help prevent cold starts.

These parameters are specified under the `cerebrium.scaling` section of your config file.

| parameter               | description                                                                                                                                                                                                                                                | type | default    |
| ----------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ---- | ---------- |
| `min_replicas`          | The minimum number of app instances to run at all times.                                                                                                                                                                                                   | int  | 0          |
| `max_replicas`          | The maximum number of instances your app can scale to.                                                                                                                                                                                                     | int  | plan limit |
| `cooldown`              | The number of seconds your app is kept warm after each request, before scaling down back to cold. The cooldown for a replica resets on receiving a new request. Once your cooldown period is reached, Cerebrium will issue a `SIGTERM` to your instance.   | int  | 60         |
| `replica_concurrency`   | The maximum number of requests an instance of your app can handle at a time. You should ensure your deployment can handle the concurrency before setting this above 1.                                                                                     | int  | 1          |
| `response_grace_period` | The maximum number of seconds Cerebrium should wait to issue a `SIGKILL` command to your instance, after issuing a `SIGTERM` command for a scale down. This is to allow you to gracefully exit your application, or to force the instance to exit quickly. | int  | 900        |

## Adding Dependencies

The dependencies section of your config file is where you can specify any dependencies you would like to install in your deployment. We support **pip**, **conda** and **apt** dependencies, and you can specify each of these in their relevant subsection of the dependencies section.

For each dependency type, you can specify the name of the package you want to install and the version constraints. If you don't want to specify any version constraints, use the `latest` keyword to install the latest version of the package.

If you have an existing **requirements.txt**, **pkglist.txt** or **conda_pkglist.txt**, you can reference these files in your config file to install the dependencies listed in them.

### pip

Your pip dependencies are specified under the `cerebrium.dependencies.pip` section of your config file.
An example of a pip dependency is shown below:

```toml
[cerebrium.dependencies.pip]
torch = ">=2.0.0"
numpy = "latest"
```

### conda

Similarly, your conda dependencies are specified under the `cerebrium.dependencies.conda` section of your config file.
An example of a conda dependency is shown below:

```toml
[cerebrium.dependencies.conda]
cuda = ">=11.7"
cudatoolkit = "11.7"
```

### apt

Finally, apt dependencies are specified under the `cerebrium.dependencies.apt` section of the config file.
These are any packages that are installed using `apt-get install` on a Linux machine.
An example of an apt dependency is shown below:

```toml
[cerebrium.dependencies.apt]
"libgl1-mesa-glx" = "latest"
"libglib2.0-0" = "latest"
```

### Using Requirements Files

An alternative to listing dependencies directly in the config file is to reference a file.
The filename is relative to the project root.

```toml
[cerebrium.dependencies.paths]
pip = "requirements.txt"
conda = "conda_pkglist.txt"
apt = "deps_folder/pkglist.txt"
```

## Config File Example

That was a lot of information.
Let's see an example of a config file in action.

Below is an example of a config file that takes advantage of all the features discussed so far.

```toml
[cerebrium.deployment]
name = "my-app"
python_version = "3.10"
include = ["./*", "main.py"]
exclude = ["./.*", "./__*"]
docker_base_image_url = "debian:bookworm-slim"
shell_commands = []

[cerebrium.hardware]
compute = "AMPERE_A10"
cpu = 2
memory = 16.0
gpu_count = 1
provider = "aws"
region = "us-east-1"

[cerebrium.scaling]
min_replicas = 0
max_replicas = 2
cooldown = 60
replica_concurrency = 1

[cerebrium.dependencies.pip]
torch = ">=2.0.0"

[cerebrium.dependencies.conda]
cuda = ">=11.7"
cudatoolkit = "11.7"

[cerebrium.dependencies.apt]
"libgl1-mesa-glx" = "latest"
"libglib2.0-0" = "latest"
```
