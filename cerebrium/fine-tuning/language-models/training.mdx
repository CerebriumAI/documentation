---
title: "Training"
description: "Commands to use with fine-tuning"
---

Below are the set of commands for you to get started with fine-tuning. Please note that all fine-tuning functionality is currently done through the terminal - frontend pending.

## Deploying a training job

Deploying a training job is done using one simple command in the Cerebrium CLI.

```bash
cerebrium train <<YOUR_JOB_NAME>> <<API_KEY>> <<HUGGINGFACE_MODEL_PATH>> <<PATH_TO_YOUR_FINETUNING_DATASET>>
```

This will create a training config and upload your job and dataset to Cerebrium to start your training on an A10 instance!

## Retrieving your most recent training jobs

Keeping track of the jobIds for all your different experiments can be challenging.  
To retrieve the status and information on your 10 most recent fine-tuning jobs, you can run the following command:

```bash
cerebrium get-training-jobs <<API_KEY>>
```

Where your API_KEY is the key for the project under which your fine-tuning has been deployed. Remember if you used the cerebrium login command you don't have to paste your API Key

# Stream the logs of your fine-tuning job

To stream the logs of a specific fine-tuning job use:

```bash
cerebrium get-training-logs <<JOB_ID>>
```

# Retrieving your training results

Once your training is complete, you can download the training results using:

```bash
cerebrium download-model <<JOB_ID>> <<API_KEY>>
```

This will return a zip file which contains your **adapter** and **adapter config** which should be in the order of 10MB for your 7B parameter model due to the extreme efficiency of PEFT fine-tuning.

# Deploy your fine-tuned model

To deploy your model you can use Cortex. Below is an example that you can simply adapt in order to get deploy your model in just a few lines of code. We will be releasing
auto-deploy functionality soon!


```python
  from transformers import AutoModelForCausalLM
  from peft import PeftModel, PeftConfig # Add the peft libraries we need for the adapter

  peft_model_id = "path/toYourAdapter"
  config = PeftConfig.from_pretrained(peft_model_id)
  model = AutoModelForCausalLM.from_pretrained(config.base_model_name_or_path)
  model = PeftModel.from_pretrained(model, peft_model_id) # Add the adapter to the model
  tokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path)

  model = model.to("cuda")
  model.eval() # set the model to inference mode

```
*Note: if you have fine-tuned a Llama based model, ensure that you are using the latest huggingface transformers release that supports Llama models as part of the AutoModelForCausalLM class.*

Now for inference, you just need to place the prompt into the template used for training. In this example we do it as follows
``` python
  template =  "### Instruction:\n{instruction}\n\n### Response:\n"
  question = template.format(instruction=prompt) 
  inputs = tokenizer(question, return_tensors="pt")

  with torch.no_grad():
    outputs = model.generate(input_ids=inputs["input_ids"].to("cuda"), max_new_tokens=10)
    print(tokenizer.batch_decode(outputs.detach().cpu().numpy(), skip_special_tokens=True)[0])

```

These adapters can be combined with others when using your model at inference time.  
For more information, see
[Using Adapter Transformers at Hugging Face](https://huggingface.co/docs/hub/adapter-transformers#exploring-adaptertransformers-in-the-hub)
