---
title: Deepgram
description: Deploy Deepgram speech-to-text services on Cerebrium
---

Cerebrium's partnership with [Deepgram](https://www.deepgram.com/) enables simple deployment of speech-to-text (STT) services with simplified configuration and independent scaling.

<Note>
Using Deepgram services requires an Enterprise Deepgram account and API key for self-hosted models. Contact Deepgram support to access this feature.

Links to the Deepgram model files referenced below (file extension `.dg`) should should be obtained from the Deepgram Account Representative.

For the purposes of this example, the `nova-2-general` model will be used.

</Note>
<Note>
  Deepgram Partner Service is available from CLI version 1.39.0 and greater
</Note>

## Setup

1. Create a simple cerebrium app with the CLI:

```bash
cerebrium init deepgram
```

2. Create a self-hosted API key from the <b>Deepgram</b> dashboard. Navigate to the **Secrets** tab in the <b>Cerebrium</b> dashboard and add the API key with the name `DEEPGRAM_API_KEY`. This secret automatically becomes available as an environment variable in the deployment.

3. ```
   Download model files from Deepgram's self-hosted section in the <b>Deepgram</b> dashboard [here](https://developers.deepgram.com/docs/deploy-deepgram-services#pull-deepgram-container-images). Select the 'license proxy' deployment type. Upload downloaded model files (with .dg extension) to persistent-storage in the `/deepgram-models` folder, which attaches to the engine container. Use this command to upload the files:
   ```

```bash
cerebrium cp <model>.dg deepgram-models/<model>.dg
```

4. Update the cerebrium.toml file to be the following - this is where you can set your hardware requirements, scaling parameters, region etc..

```toml
[cerebrium.deployment]
name = "deepgram"

[cerebrium.runtime.deepgram]

[cerebrium.hardware]
cpu = 4
region = "us-east-1"
memory = 32
compute = "AMPERE_A10"
gpu_count = 1

[cerebrium.scaling]
min_replicas = 0
max_replicas = 2
cooldown = 1800
replica_concurrency = 150
```

5. Run 'cerebrium deploy' to deploy your app. This should now give you an endpoint to use for your Deepgram service.

6. You can now use the Deepgram service in your app by calling the endpoint with the appropriate parameters such as

```curl
curl -X POST --data-binary @audio.wav "https://api.cortex.cerebrium.ai/v4/p-xxxxxx/deepgram/v1/listen?model=nova-3&smart_format=true"
```

## API Key Configuration

To use Deepgram services, you need to provide your Deepgram API key:

1. Sign up for a Deepgram account at [deepgram.com](https://www.deepgram.com/)
2. Create an API key from your Deepgram dashboard
3. Add the API key to Cerebrium:
   - Navigate to the **Secrets** tab in your Cerebrium dashboard
   - Add your Deepgram API key as either an app-specific or project-wide secret. The name of the secret should be `DEEPGRAM_API_KEY`
   - The secret will be automatically available as an environment variable in your deployment

## Scaling and Concurrency

Deepgram services support independent scaling configurations:

- **min_replicas**: Minimum number of instances to maintain (0 for scale-to-zero)
- **max_replicas**: Maximum number of instances that can be created during high load
- **replica_concurrency**: Number of concurrent requests each instance can handle
- **cooldown**: Time in seconds that an instance remains active after processing its last request

Adjust these parameters based on your expected traffic patterns and latency requirements.

## Usage Examples

Deepgram services are particularly useful for voice agent applications. For example, Cerebrium runs Deepgram STT models on the same network as the app next to LiveKit workers, which reduces latency by approximately 400ms.

For a complete example of using Deepgram in a voice agent application, see the [LiveKit Outbound Agent example](/v4/examples/livekit-outbound-agent).
