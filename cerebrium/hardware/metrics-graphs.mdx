---
title: Understanding Metrics Graphs
---

Cerebrium provides real-time metrics graphs to help you monitor your app's resource usage and performance. These graphs offer valuable insights into how your application is utilizing resources and can help you identify potential bottlenecks or optimization opportunities.

## Available Metrics

The dashboard displays the following metrics for your apps:

### Requests

The Requests graph shows the number of concurrent requests being processed by your app over time.

**What it shows:** This graph displays the number of requests waiting in the queue or currently being processed by your app. Higher values indicate increased load on your system.

**Data source:** This metric tracks the number of requests in the queue for individual containers and the total concurrent requests across all instances of your app.

**Notes:**

- Sudden spikes in the number of requests can correlate with increased start up times if your app isn't optimised to handle such spikes
- When requests spike, the app will auto-scale to handle the load - no need for user intervention

### GPU

The GPU graph shows the GPU memory usage of your app over time.

**What it shows:** This graph displays the amount of GPU memory (VRAM) being used by a single worker of your app in gigabytes. It represents the maximum memory used by a single worker across all GPUs allocated to that worker.

**Data source:** This metric is collected from NVIDIA's Data Center GPU Manager. The value is converted from megabytes to gigabytes for easier reading.

**Notes:**

- GPU memory usage includes both model weights and intermediate activations
- Memory spikes often occur during inference or when loading models
- If you're seeing values close to your GPU's total memory, you may be at risk of out-of-memory errors
- Some frameworks may not release GPU memory immediately after use, resulting in a "sawtooth" pattern
- Zero values may indicate your app isn't utilizing the GPU, which could mean your model isn't properly loaded on the GPU

### CPU

The CPU graph shows the CPU utilization of your app over time.

**What it shows:** This graph displays the maximum rate of CPU usage in cores for a single worker. For example, a value of 2.0 means a single worker of your app is using the equivalent of 2 full CPU cores.

**Data source:** This metric measures the cumulative CPU time consumed by the container and calculates the per-second usage over the specified time window.

**Notes:**

- CPU usage can spike during model loading, preprocessing, or when handling requests that don't utilize the GPU
- Values consistently near your configured CPU limit may indicate CPU bottlenecks
- The graph shows the maximum rate across all instances, not the average
- CPU throttling may occur if your app exceeds its allocated CPU resources

### Memory

The Memory graph shows the RAM usage of your app over time.

**What it shows:** This graph displays the maximum memory usage in gigabytes for a single worker of your app.

**Data source:** This metric measures the total memory used by the container. The value is converted from bytes to gigabytes for easier reading.

**Notes:**

- Memory includes both your app code, loaded models (when not on GPU), and any data being processed
- Memory usage typically increases during model loading and may decrease once models are transferred to GPU
- If memory usage approaches your configured limit, your app may be terminated with an OOM (Out of Memory) error
- Memory usage patterns can help identify memory leaks or inefficient resource usage

### Workers

The Workers graph shows the number of active replicas (instances) of your app over time.

**What it shows:** This graph displays the number of ready replicas that are currently serving traffic for your app.

**Data source:** This metric tracks the number of replicas that are ready to serve traffic for your app.

**Notes:**

- Changes in this graph reflect your app's auto-scaling behavior
- Increases indicate scaling up to handle more traffic, while decreases show scaling down during periods of lower demand
- If this value remains at 0, your app may be experiencing deployment issues
- The time it takes for this number to increase reflects your app's cold start time

## Reading Time-Series Graphs

When interpreting the metrics graphs, keep these points in mind:

- **Time ranges:** For ranges greater than 1 day, the x-axis displays dates. For ranges of 1 day or less, only times are shown.
- **Resolution:** You can adjust the resolution (Low, Medium, High) to change the granularity of the data points.
- **Zooming:** You can zoom in on a specific time period by clicking and dragging across the graph.
- **Filtering:** You can filter metrics by specific container instances to troubleshoot issues with individual replicas.

## Using Metrics for Optimization

These metrics can help you optimize your app in several ways:

- **Scaling configuration:** Analyze the Requests and Workers graphs to fine-tune your scaling parameters
- **Resource allocation:** Use the CPU, Memory, and GPU graphs to right-size your resource requests
- **Performance bottlenecks:** Identify which resources are limiting your app's performance
- **Cost optimization:** Detect over-provisioned resources that could be reduced to lower costs

For more information on configuring CPU and memory resources, see the [CPU and Memory](/cerebrium/hardware/cpu-and-memory) documentation. For GPU configuration, refer to [Using GPUs](/cerebrium/hardware/using-gpus).
