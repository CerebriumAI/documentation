---
title: Using CUDA
---

## Overview

CUDA (Compute Unified Device Architecture) enables apps to use graphics cards (GPUs) to speed up calculations. Unlike standard processors (CPUs) that handle one task at a time, graphics cards can handle many tasks simultaneously.

## How CUDA Works

CUDA connects apps to graphics cards, splitting large tasks into smaller pieces that can be processed at the same time. This makes operations like image processing and complex mathematical calculations much faster than using a standard processor alone.

## Using CUDA with app dependencies

Many python packages come with built-in CUDA support. For example, the popular machine learning package _PyTorch_ includes CUDA in its installation:

```toml
[cerebrium.dependencies.pip]
torch = "latest" # PyTorch with graphics card support
```

## Special Requirements

Some apps need direct access to CUDA system libraries and tools. The CUDA base image provides this complete CUDA toolkit environment. This is often necessary when apps:

- Compile custom CUDA code
- Access low-level CUDA features
- Need specific CUDA driver versions
- Require CUDA development tools

The base image can be set in the `cerebrium.toml` file:

```toml
[cerebrium.deployment]
docker_base_image_url = "nvidia/cuda:12.1.1-runtime-ubuntu22.04"
```

## Docker image selection

When building GPU apps, choosing the right Docker base image is crucial for balancing functionality and performance. While the default CUDA runtime image (`nvidia/cuda:12.1.1-runtime-ubuntu22.04`) provides essential CUDA capabilities, some apps may require the expanded toolset found in the `nvidia/cuda:12.1.1-runtime-ubuntu22.04` image or other specialized configurations for the development of new apps. Through the `docker_base_image_url` setting in cerebrium.toml, it is possible to specify any public Docker image that matches the app's requirements, giving flexibility to customize the build environment.

## Cold start optimisation

The size and complexity of your chosen Docker image directly impacts cold start performance – the time it takes to initialize your application from a “cold” state. Larger images with integrated development tools (Like the _devel_ image will take longer to download and initialize compared to minimal runtime images. For applications where a rapid startup is crucial, you might want to prioritize a leaner base image. However, if your application requires specific development tools or libraries, the tradeoff of longer cold starts might be necessary. Some applications can benefit from keeping instances "warm" to avoid cold starts entirely, though this approach comes with its own resource considerations.

For working examples, see the [Stable Diffusion Example](https://github.com/CerebriumAI/examples/blob/bfb2337516b0ad37a3bd31c64c41d72872f9f428/7-image-and-video/6-regular-stable-diffusion/cerebrium.toml#L9).
