---
title: Calculating compute cost
description: How to calculate the cost of your deployment on Cerebrium
---

The cost of your deployment is based on the hardware you select and the amount of time it executes for. <b>Essentially, every time we run your code, or you specify that a
machine should be running - we charge you</b>. GPU, CPU and Memory usage are all charged per second while persistent storage is charged per GB per month. You can
view the pricing of various compute on our [pricing page](https://www.cerebrium.ai/pricing)

When you deploy a model, there are two processes we charge you for:

1. We charge you for the build process in order to setup your model environment. This means we need to download your Python version, download and install Linux packages, Python packages
   and any model files you require. We only charge you for a model build once until you change your environment ie: add more packages or specify different versions - then we will charge you again since
   we rebuild the environment. This process happens every time you do 'cerebrium deploy'. However, we do a lot of caching so each subsequent build should be a lot less than the first.
2. The model runtime. This is the amount of time it takes your code to run from start to finish on each request. There are 3 costs to consider here:

- <u>Cold start</u>: This is the amount of time it takes to spin up a server(s),
  load your environment, connect storage etc. This is part of the Cerebrium
  service and something we are working on every day to get as low as possible. <b>We do not charge you for this!</b>
- <u>Model initialization</u>: This part of your code is outside of the predict
  function and only runs when your model incurs a cold start. You are charged
  for the amount of time it takes for this code to run. Typically this is
  loading a model into GPU RAM.
- <u>Predict runtime</u>: This is the code stored in your predict function and
  runs every time a request hits your endpoint

Let us go through an example of how you can calculate the cost of your deployment:

The model you wish to deploy requires:

- 24 GB VRam (A5000): $0.000356 per second
- 2 CPU cores: 2 \* $0.0000532 per second
- 20GB Memory: 20 \* $0.00000659 per second
- 10 GB persistent storage: 10 \* $0.3 per month

In our situation, your model works on the first deployment and so you incur only one build process of 2 minutes. Additionally, let's say that the model has 10 cold starts a day with an average initialization of 2 seconds and lastly and average runtime (predict) of 2 seconds. Let us calculate your
expected cost at month end with you expecting to do 100 000 model inferences.

```python
# Your variables
average_initialization_time = 2
cold_starts_per_month = 300 #10 a day for 30 days
average_inference_time = 2 # seconds
number_of_inferences = 100000 # number of inferences per month

# cost calculation
total_build_compute_cost = build_seconds * (GPU_cost + CPU_cost*num_of_cpu_cores + memory_cost*gb_of_RAM )
total_initialization_time = average_initialization_time * cold_starts_per_month
total_inference_time = average_inference_time * number_of_inferences
inference_compute_cost = (GPU_cost + CPU_cost*num_of_cpu_cores + memory_cost*gb_of_RAM ) * total_inference_time
storage_cost = gb_of_persistent_storage * persistant_storage_cost

total_cost = inference_compute_cost + storage_cost + total_build_compute_cost + total_initialization_time


print(f"Build Compute cost: ${total_build_compute_cost :.2f}/month",
      f"Initialization Compute cost: ${total_initialization_time :.2f}/month",
      f"Inference Compute cost: ${total_inference_time :.2f}/month",
      f"\nStorage cost: ${storage_cost :.2f}/month",
      f"\nTotal cost: ${total_cost :.2f}/month")
```

From this, you can see that the cost of your deployment is:

```bash
Build Compute cost: $0.07/month
Initialization Compute cost: $0.357/month
Inference Compute cost: $118.84/month
Storage cost: $3.00/month
Total cost: $121.567/month
```

Now you would expect the cost to grow linearly with the number of requests you get however, we want to help you grow your business and so we are happy to discount your computing cost as you grow.
To see the discounts you can get for your volume, please contact us directly on [Sales](mailto:sales@cerebrium.ai)

As you grow, don't worry about scaling your deployment with demand, sourcing new hardware or managing your infrastructure. You can focus on your code and we'll handle the rest.
