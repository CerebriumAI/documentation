---
title: "Training Diffusion Models"
description: "Guide to fine-tuning a diffusion model on Cerebrium"
---

## Running a diffuser fine-tuning

Diffusion models are incredible at generating images from art, designs, logos and beyond. Fine-tuning stable diffusion models allows you to create your very own personal model which generates images of your object or in your style.  
Using Cerebrium's trainer, you can fine-tune your model with one command using your very own data without having worry about any of the hardware or implementation details!

Our diffusion fine-tuner leverages parameter efficient finetuning (PEFT) and low-rank adaptors (LORA) to only train a small subset of the diffusion model. This drastically improves training time while focusing on the parts of the model that make the biggest difference to your results, saving you time while yielding results comparable to full network finetuning.

While in the beta phase, the following models are supported for fine-tuning on Cerebrium:

| Model Name                           | Huggingface Path               |
| ------------------------------------ | ------------------------------ |
| Stable Diffusion V1.5                | runwayml/stable-diffusion-v1-5 |
| Stable Diffusion V2.1 (coming soon!) | stabilityai/stable-diffusion-2 |

We're actively working on adding more models based on demand so please let the team know if there's a model you would like us to add support for.

### Starting a job with the CLI

Starting a job on cerebrium requires four things:

- A name for you to use to identify the training job.
- Your API key.
- A config file or json string. See [this section](#config-files) for more info.
- Your dataset of training images along with the corresponding prompt.

Once you have these, you can start a fine-tuning job using the following command:

```bash
cerebrium train --name <<Name for your training>> --training-tupe "diffuser" --api-key <<Your api key if you haven't logged in>> --config-file <<Path to your config file>>
```

Your `config-file` or `config-string` could alternatively provide all the other parameters.

#### Monitoring

Once your training job has been deployed, you will receive a **job-id\*\***. This can be used to access the job status as well as retrieve the logs.

Checking the training status:

```bash
cerebrium get-training-jobs --last-n <optional parameter to only return the status of the last n runs>
```

Retrieving the training logs can be done with:

```bash
cerebrium get-training-logs <<Your training job ID>>
```

_Note that your training logs will only be availible once the job has started running and will not be stored after it is complete._

_Coming soon: logging your training with **weights and biases** is in the final stages of development._

## Retrieving your training results

Once your training is complete, you can download the training results using:

```bash
cerebrium download-model <<JOB_ID>> <<API_KEY>>
```

This will return a zip file which contains your attention processors and the validation images generated by your model during training.

## Using your Fine-tuned Diffuser

Using your finetuning results is done as follows:

```python

from diffusers import (
    DiffusionPipeline,
    DPMSolverMultistepScheduler,
)
import torch

# Boilerplate loading of model
pipeline = DiffusionPipeline.from_pretrained(
    your_model_name, revision=your_model_revision, torch_dtype=torch.float16
)
pipeline.scheduler = DPMSolverMultistepScheduler.from_config(pipeline.scheduler.config)
pipeline = pipeline.to("cuda")


# LOAD IN YOUR TRAINING RESULTS
# load attention processors from where they are saved in your_results/checkpoints/final/attn_procs/pytorch_lora_weights.bin
pipeline.unet.load_attn_procs(os.path.join(final_output_dir, "attn_procs"))
# And that's all you need to do to load in the finetuned result!
# Now you can run your inference as you would like with the pipeline.


# some inference variables
your_prompt = "Your training prompt that you would like to use here"
num_images =  4 # number of images to generate
your_manual_seed = 42 # a manual seed if you would like repeatable results

# run inference as you normally would
generator = torch.Generator(device="cuda").manual_seed(your_manual_seed)
images = [
    pipeline(your_prompt, num_inference_steps=25, generator=generator).images[0]
    for _ in range(num_images)
]

```


# Example yaml config file
```yaml
#############################################
#  Mandatory Parameters
#############################################

#############################################
#  Optional Parameters
#############################################
# Diffuser params
# revision: "main" # Revision of the diffuser model to use.
# validation_prompt: ~ # an optional validation prompt to use. If ~, will use the training prompt.
# custom_tokenizer: "" # custom tokenizer from AutoTokenizer if required.

# # Dataset params
# train_image_dir:  data/training_class_images/ # Directory of training images.
# prior_class_image_dir: ~ # "data/prior_class_images" # Optional directory of images to use for prior class.
# prior_class_prompt: "Photo of a dog."

# # Training params
# training_args:
#   # General training params
#   num_validation_images: 4 # Number of images to generate in validation.
#   learning_rate: 1e-4
#   num_train_epochs: 10
#   seed: ~
#   resolution: 512 # Resolution to train images at.
#   center_crop: False # Whether to center crop images to resolution.
#   train_batch_size: 2
#   num_prior_class_images: 10
#   prior_class_generation_batch_size: 2
#   prior_loss_weight: 1.0 # Weight of prior loss in the total loss.
#   max_train_steps: ~ # maximum training steps which overrides number of training epochs
#   validation_epochs: 5 # number of epochs before running validation and checkpointing

#   # Training loop params
#   gradient_accumulation_steps: 1
#   learning_rate: 0.0005
#   lr_scheduler: "constant"
#   lr_warmup_steps: 5
#   lr_num_cycles: 1
#   lr_power: 1.0
#   allow_tf32: False
#   max_grad_norm: 1.0
#   mixed_precision: "fp16"
#   prior_generation_precision: ~
#   scale_lr: False
#   use_8bit_adam: True
#   use_xformers: True # Whether to use xformers memory efficient attention or not.
``` 

