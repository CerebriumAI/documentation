---
title: Running Once-off workloads
description: "Cerebrium Run is a command-line tool that allows you to run once-off workloads on the Cerebrium platform"
---

## Introduction

<Note>
  This feature is in the beta stages of development and is being actively worked
  on. If you have any feedback or suggestions, please reach out to us on our
  communities on Slack or Discord!
</Note>

The `cerbrium run` is a command-line tool that allows you to run once-off workloads on the Cerebrium platform. This feature is particularly useful for executing tasks such as creating embeddings, training models, and more, without the need to set up a full deployment.

To get started, ensure your project includes a `main.py` file. This is the entry point that the `cerebrium run` command will execute and is vital for your code to execute.

If you're new to Cerebrium or need a refresher on setting up the Cerebrium Command Line Interface (CLI), start by checking out our easy-to-follow [installation guide](/cerebrium/getting-started/installation).
Once you have the CLI setup, navigate to the folder containing your deployment's `main.py` and `cerebrium.toml` config file and simplyrun the following command:

```bash
cerebrium run
```

### Optional Arguments

The `cerebrium run` command supports a variety of optional arguments to customize your run.  
These arguments are the same as the ones you would use with the `cerebrium deploy` command, and are detailed in the [config-files](/cerebrium/environments/config-files) section.

### Using `run`` with cortex deployments

If you are running a cortex deployment, and would like to pass in data to your predict function, you can do so by using the `--data` flag. This flag takes in a JSON string, similar to the body of your inference request to a `cortex` deployment. This data will be passed to your predict function as a dictionary.

```bash
cerebrium run --data '{"key": "value"}'
```

## Persistent Storage

Utilizing the `cerebrium run` command, you can perform a variety of operations such as creating embeddings, training models, and more.
Once these operations are completed, you have the option to save the resulting data, including model weights, embeddings, logs, or any other relevant information, directly to your project's persistent storage.
This feature is particularly beneficial for preserving important data across different runs.

The persistent storage acts as a shared resource across all deployments within your project, enabling seamless data sharing and access.
To leverage this, simply direct your data to be saved in a path prefixed with `/persistent-storage`.
This data can then be effortlessly accessed or loaded from the same path in any of your project's deployments, facilitating a highly efficient and collaborative environment for managing and utilizing your machine learning assets.

## Retrieving Results

When your run is complete, the returned results will be displayed alongside any prints or logs from your run in the terminal.

## Example

Here's an example of a simple `main.py` file that you can use to test the `cerebrium run` command:

```python
# main.py
import pyfiglet

ascii_banner = pyfiglet.figlet_format("Hello World!!")
print(ascii_banner)
print("\nDONE!\n")
```

In our `cerebrium.toml` file, we add **pyfiglet** to the `pip` requirements section as follows:

```toml
[cerebrium.dependencies.pip]
pyfiglet = "latest"
```

Now, when we run the `cerebrium run` command, we should see the following output:

![Demo of the runner functionality](/images/runner/runner-basic-demo.png)

## Roadmap

The current features are just the beginning of what we have planned for Cerebrium Run.

Here are some of the features we are working on:

- Webhook endpoints for run completion or results
- Large file retrievals from runs
