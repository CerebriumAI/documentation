---
title: "Available Hardware"
description: "A list of hardware that is available on Cerebrium's platform."
---

The Cerebrium platform allows you to quickly and easily deploy your code on a variety of different hardware.
We handle all the hard work of setting up the hardware and making sure that your code runs on it so that you don't have to. Everything from the hardware drivers to the scaling of your deployments is managed by us so that you can focus on what matters most: your code.

This page lists the hardware that is currently available on the platform.

# Hardware

## Graphics Cards

We have the following graphics cards available on the platform:
| Name | Memory | Minimum Plan | Max fp32 Model Params | Max fp16 Model Params | Max fp8 Model Params |
| --------------------------------------------------------------------------------------------------- | :------: |------ | :-------------------: | :-------------------: | :------------------: |
| [NVIDIA H100](https://www.nvidia.com/en-us/data-center/h100/) | 80GB | Enterprise | 18B | 36B | 72B |
| [NVIDIA A100](https://www.nvidia.com/en-us/data-center/a100/) | 80GB | Enterprise | 18B | 36B | 72B |
| [NVIDIA A100](https://www.nvidia.com/en-us/data-center/a100/) | 40GB | Standard | 9B | 18B | 36B |
| [NVIDIA RTX A6000](https://www.nvidia.com/en-us/design-visualization/rtx-a6000/) | 48GB | Hobby | 10B | 21B | 43B |
| [NVIDIA RTX A5000](https://www.nvidia.com/en-us/design-visualization/rtx-a5000/) | 24GB | Hobby | 5B | 10B | 21B |
| [NVIDIA RTX A4000](https://www.nvidia.com/en-us/design-visualization/rtx-a4000/) | 16GB | Hobby | 3B | 7B | 14B |
| [NVIDIA Quadro RTX 5000](https://www.nvidia.com/en-us/design-visualization/quadro/quadro-rtx-5000/) | 16GB | Hobby | 3B | 7B | 14B |
| [NVIDIA Quadro RTX 4000](https://www.nvidia.com/en-us/design-visualization/quadro/quadro-rtx-4000/) | 8GB | Hobby | 1B | 3B | 7B |
| [NVIDIA A10](https://www.nvidia.com/en-us/data-center/a10-tensor-core-gpu/) (_Depricating soon_) | 24GB | Hobby | 5B | 10B | 21B |
| [NVIDIA T4](https://www.nvidia.com/en-us/data-center/tesla-t4/) (_Depricating soon_) | 16GB | Hobby | 3B | 7B | 14B |

_NOTE: The maximum model sizes are calculated as a guideline and are assuming that the model is the only thing loaded into VRAM. Longer inputs will result in a smaller maximum model size. Your milage may vary._

These GPUs can be selected using the `--hardware` flag when deploying your model on Cortex or by using the `hardware` parameter when deploying your model with the Conduit.  
For more help on deciding which GPU you require, see this section [here](#choosing-a-gpu).


## CPUs

We select the CPU based on your choice of hardware, choosing the best available options so you can get the performance you need.

You can choose the number of CPU cores you require for your deployment. If you don't need the cores, choose how many you require and pay for only what you need!

## CPU Memory

We let you select the amount of memory you require for your deployment.  
All the memory you request is dedicated to your deployment and is not shared with any other deployments, ensuring that you get the performance you need.
This is the amount of memory that is available to your code when it is running and you should choose an adequate amount for your model to be loaded into VRAM if you are deploying onto a GPU.
Once again, you only pay for what you need!


## Storage

We provide you with a persistent storage volume that is attached to your deployment.
You can use this storage volume to store any data that you need to persist between deployments. Accessing your persistent storage is covered in depth for [conduit here](/cerebrium/conduit/advanced-functionality/persistent-memory) and [cortex here](/cerebrium/cortex/advanced-functionality/persistent-storage).

The storage volume is backed by high-performance SSDs so that you can get the best performance possible
Pricing for storage is based on the amount of storage you use and is charged per GB per month.


# Determine your Hardware Requirements

Deciding which hardware you require for your deployment can be a daunting task.  
On one hand, you want the best performance possible but on the other hand, you don't want to pay for more than you need. 

## Choosing a GPU
Choosing hardware can be a complicated task of calculating VRAM usage based on the number of parameters you have as well as the length of your inputs. Additionally, there are variables that are dependent on your inputs to your model which will affect the VRAM usage substantially. For example, with LLMs and transformer-based architectures, you need to factor in attention processes as well as any memory-heavy positional encoding that may be happening with can increase VRAM usage exponentially for some methods. Similarly, for CNNs, you need to look at the number of filters you are using as well as the size of your inputs. 

As a rule of thumb, the easier way is to choose the hardware that has at least $1.5 \times$ the minimum amount of VRAM that your model requires. 
This approach is conservative and will ensure that your model will fit on the hardware you choose even if you have longer inputs than you expect. However, it is just a rule of thumb and you should test the VRAM usage of your model to ensure that it will fit on the hardware you choose.

You can calculate the VRAM usage of your model by using the following formula:
$$ VRAM_{model} = numParams \times \#bytesPerDataType$$

For example, if you have a model that is 7B parameters and you decide to use 32-bit Floating point precision, you can calculate the VRAM usage as follows:
$$ VRAM_{model} = 7B \times 4 = 28GB$$

When you include the $1.5\times$ multiplier from our rule of thumb, this means that you should choose a GPU with at least ~40GB of VRAM to ensure that your model will fit on the hardware you choose.  

Alternatively, if you were happy with the slight precision penalty of using quantisation, your model would have required 7GB of VRAM for 8-bit quantisation. So you could have chosen a GPU with 16GB of VRAM. This is the approach we recommend especially with large models (>20B parameters) as the precision penalty is minimal and your cost savings are substantial.

<Note>
Pro tip: The precision loss from quantisation is negligible in comparison to the performance gains you get from the larger model that can fit on the same hardware.
</Note>

## Setting your number of CPU Cores
If you are unsure of how many CPU cores you need, it's best to start with either 2 or 4 cores and scale up if you find you need more. For most use cases, 4 cores are sufficient. 


## Calculating your required CPU Memory

A guideline for calculating the amount of CPU memory that you require is to use the same amount of CPU memory as you have VRAM. This is because the model will generally be loaded into CPU memory first during initialisation. This happens while the weights are being loaded from storage and the model is being compiled. Once the model is compiled, it is loaded into VRAM and the CPU memory is freed up.  
Certain libraries, such as *transformers*, do have methods that allow you to use less CPU memory by setting flags such as `low_cpu_mem_usage` which trade off CPU memory for longer initialisation times. 


## Storage Requirements
Storage is calculated on an ad-hoc basis and is calculated as you use it.  
This storage is persistent and will be available to you for as long as you need it. 

