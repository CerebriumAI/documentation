---
title: "HuggingFace Transformers"
description: "This example provides the steps to deploy a HuggingFace Transformers model
using Cerebrium."
---

## Intro

Cerebrium supports [HuggingFace Transformers](https://huggingface.co/) through the use of [`pipeline`](https://huggingface.co/docs/transformers/main_classes/pipelines).
By using `pipeline` we are able to load both the tokenizer and model easily within the same object. 

Using the Transformers library, you have instant access to pre-trained models for NLP tasks. If you would like to deploy HuggingFace models that aren't supported
by pipeline, I suggest you look at using our [custom python functionality](/quickstarts/custom)

By the end of this guide you'll have an API endpoint that can handle any scale
of traffic by running inference on serverless CPU's/GPU's.

### Project set up

Before building you need to set up a Cerebrium account. This is as simple as
starting a new Project in Cerebrium and copying the API key. This will be used
to authenticate all calls for this project.

### Create a project

1. Go to [dashboard.cerebrium.ai](https://dashboard.cerebrium.ai)
2. Sign up or Login
3. Navigate to the API Keys page
4. You should see an API key with the source "Cerebrium". Click the eye icon to
   display it. It will be in the format: c_api_key-xxx

![API Key](/images/quickstarts/onnx-1.png)

### Develop model

Now navigate to where your model code is stored. This could be in a notebook or
in a plain `.py` file.

To start you should install the Cerebrium framework running the following
command in your notebook or terminal

```bash
pip install cerebrium
```

Using a HuggingFace model is as simple importing the `HUGGINGFACE_PIPELINE` model type and passing in the *task* and *model id*.
In this case, we will use the `EleutherAI/gpt-neo-125M` model to generate text.

```python
from cerebrium import Conduit, model_type, hardware

# Create a conduit
c = Conduit(
    'hf-gpt',
    '<API_KEY>',
    [
        (model_type.HUGGINGFACE_PIPELINE, {"task": "text-generation", "hf_id": "EleutherAI/gpt-neo-125M"}),
    ],
)
```

Then, simply call the `deploy` method to deploy your model. HuggingFace models will typically take a few minutes to deploy.

```python
c.deploy()
```

![Deployed Model](/images/quickstarts/hf-2.png)

Your model is now deployed and ready for inference!
Navigate to the dashboard and on the Models page you will see your model.

You can run inference using `curl`.

```bash
curl --location --request POST '<ENDPOINT>' \
--header 'Authorization: <API_KEY>' \
--header 'Content-Type: application/json' \
--data-raw '[<INPUT_DATA>]'
```

Your input data should be in the same shape and typing that the HF pipeline would accept for the particular model you have chosen.
In this case, we should pass in a string. For input data of `["this is a test"]`, the response will be:

![ThunderClient Response](/images/quickstarts/hf-3.png)

Navigate back to the dashboard and click on the name of the model you just
deployed. You will see an API call was made and the inference time. From your
dashboard you can monitor your model, roll back to previous versions and see
traffic.

With one line of code, your model was deployed in seconds with automatic
versioning, monitoring and the ability to scale based on traffic spikes. Try
deploying your own model now or check out our other frameworks.
