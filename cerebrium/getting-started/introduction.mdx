---
title: "Introduction"
description: "Cerebrium is a serverless GPU infrastructure provider. We want to help you run machine learning models in the cloud efficiently and at scale."
---

Our goal is to help companies create value through machine learning as quickly and as painlessly as possible by abstracting away a lot of the complexity and mundane infrastructure setup.

We are releasing changes weekly based on your feedback so please let us know if there is something missing from our platform.
You can send us feedback requests at [support@cerebrium.ai](mailto:support@cerebrium.ai) or let us know on our [Slack](https://join.slack.com/t/cerebriumworkspace/shared_invite/zt-1qojg3eac-q4xyu5O~MeniNIg2jNeadg) and [Discord](https://discord.gg/ATj6USmeE2) communities

## How we do this

- <b>
    <u>Infrastructure</u>
  </b>
  We abstract away all of the complexity around infrastructure so you don't have
  to worry about CPUs/GPUs, Kubernetes, queues, monitoring, scaling etc. We take
  care of this to create a robust and seamless developer experience.
- <b>
    <u>Research</u>
  </b>
  We try to implement the latest research towards your model as best we can in order
  for you to deliver the best experience to your users. Besides giving you to the
  option to select the best chip for your workload, we look to see how we can take
  maximum advantage of the GPU to get your model to run faster and cheaper without
  sacrificing performance.

## Our users favorite features

- \<5 second cold-start times
- Wide variety of GPUs
- Automatic scaling from 1 to 10k requests in seconds
- Define pip/conda container environments in code
- Secrets manager
- One-click deploys
- Persistant Storage

<b>All of this in just a few lines of code!</b>

To get started, head to our [installation page](/installation)
