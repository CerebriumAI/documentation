---
title: "Segment Anything Example"
description: "To deploy Meta's segment anything model to classify clothes"
---

In this tutorial we will demonstrate how to get Meta's 'Segment Anything' model up and running in order to recognize items a person is wearing - think about the
shopping feature on Instagram. It identifies clothes and associates the link to these items.

##Basic Setup
It is important to think of the way you develop models using Cerebrium should be identical to developing on a virtual machine or Google Colab.

To start we need to create a main.py file which will contain our main Python code. This is a relatively simple implementation so we can do everything in 1 file.

I then go straight to the [Meta Repo](https://github.com/facebookresearch/segment-anything) in order to see how I can run this code.
I need to install a few dependencies. In order to do this, I need to create a requirements.txt file. with the following dependencies: 

```
git+https://github.com/facebookresearch/segment-anything.git
opencv-python
pycocotools
```

I then define the following in my main.py file

```python
from segment_anything import SamAutomaticMaskGenerator, sam_model_registry
import cv2

sam = sam_model_registry["default"](checkpoint="./sam_vit_h_4b8939.pth")
sam.to("cuda")
mask_generator = SamAutomaticMaskGenerator(
    model=sam,
    points_per_side=32,
    pred_iou_thresh=0.96,
    stability_score_thresh=0.92,
    crop_n_layers=1,
    crop_n_points_downscale_factor=2,
    min_mask_region_area=100,  # Requires open-cv to run post-processing
)
```

We define the model at the top of the file because code loaded in the top of the file is automatically run when deployed, meaning the model can be downloaded and stored within the image. You want this to happen on model
load. If this was in a function, it would have to do a lookup in cache every time a request is made and would load the model into memory which we don't want.

With the SAM model, you are able to set parameters about the how precise or how detailed you would like the model to perform. Since our use case is around detecting 
clothes on an image, we don't want the segmentation to be that grandular. You can look from Meta's documentation the different parameters you can set.

In the documentation we need to select a model type and a checkpoint. Meta recommends three - We will just use the default. I then download the ViT-H SAM model
checkpoint. You can simply add this checkpoint file to the same directory as your main.py and when you deploy your model, Cerebrium will automatically upload it.

I then would like the user to send in a few parameters for the model via API and so I need to define what the request object would look like:

```python
from pydantic import BaseModel
from typing import Optional

class Item(BaseModel):
    image: Option[str]
    file_url: Optional[str]
    coordinates: list

```

Pydantic is a data validation library and BaseModel is where Cerebrium keeps some default parameters like "webhook_url" that allows a user to send in a webhook url
and we will call it when the job has finished processing - this is useful for long running tasks. Do not worry about that functionality for this tutorial. The reason
the user is sending in a image or a file url is giving the user a choice to send in a base64 encoded image or a publicly accessible file_url we can download.

##Identifying and classifying objects

Next we would like to create three functions with the following functionality:
1. Based on the image coordinates sent in, we would like to find the annotation (segment) the user is most likely referring to.
2. Take the annotation of the image we have focused on from Meta's model, create a new image and crop it to only be a image of the the item we are focusing on.
The reason we crop the image is to make the classification model perform more accurately.
3. Pass this new image into a Object classification model so we can be told what this image is.

```Python

##1.
def find_annotation_by_coordinates(annotations, x, y):
    for ann in annotations:
        bbox_x, bbox_y, bbox_w, bbox_h = ann['bbox']
        if bbox_x <= x <= bbox_x + bbox_w and bbox_y <= y <= bbox_y + bbox_h:
            return ann
    return None

##2.
def create_image(ann):
  m = ann['segmentation']
  resized_original_image = cv2.resize(image, (m.shape[1], m.shape[0]))
  mask = np.ones((m.shape[0], m.shape[1], 3), dtype=np.uint8) *255
  mask[m] = resized_original_image[m]  # Set the segmented area to white
  x, y, w, h = ann['bbox']
  cropped_image = mask[y:y+h, x:x+w]

  return cv2.cvtColor(cropped_image, cv2.COLOR_RGB2BGR)

##3.
from transformers import ViTImageProcessor, ViTForImageClassification
processor = ViTImageProcessor.from_pretrained('google/vit-base-patch16-224')
model = ViTForImageClassification.from_pretrained('google/vit-base-patch16-224')

def classify(image):
    inputs = processor(images=image, return_tensors="pt")
    outputs = model(**inputs)
    logits = outputs.logits
    # model predicts one of the 1000 ImageNet classes
    predicted_class_idx = logits.argmax(-1).item()
    return model.config.id2label[predicted_class_idx]

```

In the last function, we implement a Vision Transformer model from Google that helps us identify objects and returns the class mostly likely predicted. You will notice
we don't define the processor and model inside the function since we only want the loading of these models to happen once so define it near the top of your
file under your sam code.

###Putting it all together

We would then like to create a function that we would like to be called every time our user submits a request. This request will download/convert the image
the user submitted, feet it into the model and return the items detected in the image. Our code looks as follows:

```python
import requests
from PIL import Image
from io import BytesIO
import base64
from models.common.decorators import worker

def download_image(url):
    if url:
        r = requests.get(url)
    else:
        return ValueError("No valid url passed")

    return Image.open(BytesIO(r.content))

@worker
def predict(item: Item):

    if (not item.image and not item.file_url): return ""

    if item.image:
        image = v2.cvtColor(Image.open(BytesIO(base64.b64decode(item.image))), cv2.COLOR_BGR2RGB)
    elif item.file_url:
        image = v2.cvtColor(download_image(item.file_url), cv2.COLOR_BGR2RGB)

    masks = mask_generator.generate(image)
    selected_annotation = find_annotation_by_coordinates(masks, cursor_x, cursor_y)

    if not selected_annotation:
        return {"message": "No annotation found at the given coordinates."}
    segmented_image = create_image(selected_annotation)
    results = classify(segmented_image)

    return {"result": result}
```

In the above code we do a few things:
1. We import the various packages we need add the necessary ones to our requirements.txt **
2. We create a function that will download the image from a url if the user supplied one.
3. We specify the @worker decorator to tell us this will be the function our endpoint runs. From this function you can always refer to other functions and classes.
2. We run our "Segment Anything" model with the image the user sent through the request.
4. Lastly, we responded with the converted images. All returns from your endpoint function must be of type list or JSON.

We can then deploy our model with the following line of code

```
cerebrium-deploy -n stable-diffusion -h A10 -m 16 -k <API_KEY>
```

After a few minutes, your model should be deployed and an endpoint should be returned. Let us create a CURL request to see the response

