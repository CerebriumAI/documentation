---
title: Running Once-off workloads
description: "Cerebrium Run is a command-line tool that allows you to run once-off workloads on the Cerebrium platform"
---

## Introduction

<Note>
  This feature is in the beta stages of development and is being actively
  worked on. If you have any feedback or suggestions, please reach out to us on
  our communities on Slack or Discord!
</Note>

The `cerbrium run` is a command-line tool that allows you to run once-off workloads on the Cerebrium platform. This feature is particularly useful for executing tasks such as creating embeddings, training models, and more, without the need to set up a full deployment.

To get started, ensure your project includes a `main.py` file. This is the entry point that the `cerebrium run` command will execute and is vital for your code to execute.

If you're new to Cerebrium or need a refresher on setting up the Cerebrium Command Line Interface (CLI), start by checking out our easy-to-follow [installation guide](/cerebrium/getting-started/installation).
Once you have the CLI setup, navigate to the folder containing your deployment's `main.py` and `cerebrium.toml` config file and simplyrun the following command:

```bash
cerebrium run
```

### Optional Arguments
The `cerebrium run` command supports a variety of optional arguments to customize your run.  
These arguments are the same as the ones you would use with the `cerebrium deploy` command, and are detailed in the [config-files](/cerebrium/environments/config-files) section.

### Using `run`` with cortex deployments

If you are running a cortex deployment, and would like to pass in data to your predict function, you can do so by using the `--data` flag. This flag takes in a JSON string, similar to the body of your inference request to a `cortex` deployment. This data will be passed to your predict function as a dictionary.

```bash
cerebrium run --data '{"key": "value"}'
```

## Using Results

When your run is complete, the returned results will be displayed alongside any prints or logs from your run in the terminal.

Alternatively, your persistent storage serves as a common resource for all deployments within a project, allowing for easy data sharing and access. 
To use your data with a normal cortex deployment, save your data to a path that starts with `/persistent-storage`.  
This data is then accessible from the same path across any deployment in the project, streamlining the use of machine learning assets.


## Example

Here's an example of a simple `main.py` file that you can use to test the `cerebrium run` command:

```python
# main.py
import pyfiglet

ascii_banner = pyfiglet.figlet_format("Hello World!!")
print(ascii_banner)
print("\nDONE!\n")
```

In our `cerebrium.toml` file, we add **pyfiglet** to the `pip` requirements section as follows:

```toml
[cerebrium.dependencies.pip]
pyfiglet = "latest"
```

Now, when we run the `cerebrium run` command, we should see the following output:

![Demo of the runner functionality](/images/runner/runner-basic-demo.png)


## Roadmap

The current features are just the beginning of what we have planned for Cerebrium Run.

Here are some of the features we are working on:

- Webhook endpoints for run completion or results
- Large file retrievals from runs
