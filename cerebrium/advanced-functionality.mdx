---
title: "Advanced Functionality"
---

## Deploying model ensembles

With Cerebrium you are also able to deploy a sequence of models simply by
specifying a list of tuples of model types and model files. Currently, this
functionality is supported across all model types, with the caveat that we
assume the models are evaluated in the order they are supplied. For example, if
you have a PyTorch model that takes in a 2D image and outputs a 1D vector, and
you have a XGB model that takes in a 1D vector and outputs a single class
prediction, you can deploy them as a sequence of models by supplying the
following to the `deploy` function:

```bash
model_flow = [(model_type.TORCH, 'torch.pt'), (model_type.XGBOOST_CLASSIFIER, 'xgb.json')]
endpoint = deploy(model_flow, 'my-flow', "<API_KEY>")
```

We are currently working on a more robust way to specify the input and output
shapes of your models or supply ways to specify pre-processing functions,
so that you can deploy more complex flows. If you have any suggestions or
feedback, please reach out to us.


## Processing Functions
In many cases, you may want to process the data in your single or multi-model flow before returning it to the next flow stage or to the user. To do this, you can supply a both a pre-processing and a post-processing function to your flow, `pre` and `post` respectively. 
The pre-processing will be applied to the input of the model, while the post-processing function will be applied to the output of your model before it is returned to the next flow stage. 
You will **need** to implement both of these functions if you wish to use binary files along with your imput data.

In particular:
- The pre-processing function should take 1-2 arguments **in order**: the input to your model and any input files as binary data. It should return a single value, which is the pre-processed input to your model. It is <b>crucial</b> that the output of your pre-processing function is the same shape and type as the input of the model in the flow stage.
- The post-processing function should take a 1-3 arguments **in order**: the output of your model, the original input data and the original binary input files. It should return a single value, which is the post-processed output of your model. You should ensure the return shape and type is the same as the input shape and type of the next flow stage. If you are at the end of your flow, you should ensure the return type is one of [list, dict, numpy.array, torch.tensor]. Note if you wish to use files, you should ensure your function signature has 3 arguments, regardless of whether you use the second argument.

For example, if you have a model that outputs a 1D vector of probabilities, you may want to return the argmax. You can do this by supplying the following function to the `deploy` function with the last flow stage:

```python
# This function will be applied to the input of your model
def pre_process(data, files):
    # data is a list of input data, files is a list of binary objects
    import numpy as np
    from PIL import Image
    labelled = {d: Image.open(f) for d, f in zip(data, files)}
    return labelled
def post_process(result, input_data, files):
    # result is the output of your model, input_data is the original input data from the 1st flow stage, files is the original binary input files
    import numpy as np
    labels = np.argmax(result, axis=1)
    return {d: l for d, l in zip(input_data, labels)}
model_flow = [(model_type.TORCH, 'torch.pt'), (model_type.XGBOOST_CLASSIFIER, 'xgb.json', {"pre": pre_process, "post": post_process})]
endpoint = deploy(model_flow, 'my-flow', "<API_KEY>")
```
You may use functions inside your processor from the following libraries:
- `numpy`
- `scikit-learn`
- `torch`
- `pandas`
- `Pillow`
- `transformers`
We are expanding this list of libraries, so if you have a specific library you would like to use, please let us know!

<Note>
  Note that any imports you need to do in your processing functions must be done inside the function. This is because the function is serialized and sent to the Cerebrium servers, and the imports will not be available on the server.
</Note>

## Using Files as Input Data
You may also use files as input data to your model. 
This is particularly useful if you have a model that takes in images, audio files, or other binary data. 
To do this, you will need to supply at least an initial pre-processing function. 
You can then use the files in your model as you would any other input data.
It is important to note that `conduit.run` expects the files to be passed in as **binary data** on the cloud, so you will need to open the files and read them in as binary data before passing them to test with `conduit.run` locally.

```python
def pre_process(data, files):
    import numpy as np
    from PIL import Image
    labelled = {d: Image.open(f) for d, f in zip(data, files)}
    return labelled

from cerebrium import Conduit, model_type
conduit = Conduit('<MODEL_NAME>', '<API_KEY>', [('<MODEL_TYPE>', '<MODEL_FILE>')], {"pre": pre_process"})
conduit.load('./')
conduit.run(data, files) # files should be a list of binary objects
conduit.deploy()
```

### Request and Response with Forms

<RequestExample>
```bash Request
  curl --location --request POST '<ENDPOINT>' \
      --header 'Authorization: <API_KEY>' \
      --header 'Content-Type: 'multipart/form-data' \
      -F data='[<DATA_INPUT>]' \
      -F files=@<FILES_PATH>
```
</RequestExample>
The parameters to a form request are similar to a JSON request, however both the `data` and `files` are passed in as *form data*.

<ParamField header="Authorization" type="string" required>
  This is the Cerebrium API key used to authenticate your request. You can get it from your Cerebrium dashboard.
</ParamField>
<ParamField body="Content-Type" type="string" required>
   The content type of your request. Must be multipart/form-data for forms.
</ParamField>
<ParamField form="data" type="string" required>
   A stringified list of data points you would like to send to your model. e.g. for 1 data point of 3 features: '[[1,2,3]]'.
</ParamField>
<ParamField form="files" type="file" required>
   A file or list of files you would like to send to your model. The files should be in the same order as the data points in the `data` field.
</ParamField>


<ResponseExample>

```json Response
{
  "result": [<MODEL_PREDICTION>],
  "run_id": "<RUN_ID>",
  "run_time_ms": <RUN_TIME_MS>
  "prediction_ids": ["<PREDICTION_ID>"]
}
```
</ResponseExample>

#### Response Parameters
<ResponseField name="result" type="array" required>
  The result of your model prediction.
</ResponseField>
<ResponseField name="run_id" type="string" required>
  The run ID associated with your model predictions.
</ResponseField>
<ResponseField name="run_time_ms" type="float" required>
  The amount of time if took your model to run down to the millisecond. This is what we charge you based on.
</ResponseField>
<ResponseField name="prediction_ids" type="array" required>
  The prediction IDs associated with each of your model predictions. Used to track your model predictions with monitoring tools.
</ResponseField>
