---
title: "Dataset Curation"
description: "Advice on how best to structure your datasets"
---

<Note>
  Cerebrium's fine-tuning functionality is in public beta and we are adding more
  functionality each week! If there are any issues or if you have an urgent
  requirement, please reach out to [support](mailto:support@cerebrium.ai)
</Note>

The fine-tuning library has been built to leverage Parameter Efficient Fine-tuning and Low-Rank Approximation to reduce the number of trainable parameters by >99.9% while providing results that are comparable to full fine-tuning.  
Due to the much lower number of trainable parameters, the datasets used do not need to be greater than 1000 examples however, it's always best to use as many, diverse training examples as possible to cover a wide variety of use cases.

By default, we provide two templates for question-and-answering which we have adapted from [Alpaca-Lora format](https://github.com/TianyiPeng/alpaca-lora/tree/main/templates) as our prompt template format. You can use these templates or create your own. For more information on templating see [this page](/cerebrium/fine-tuning/language-models/custom-templates).
For your convenience, an example dataset has been provided for testing [here](../data/training-dataset-example.json). The dataset is
a JSON or JSONL file which contains a "prompt", "completion", and if needed "context" parameters.

Below are some helpful tips to take into consideration when preparing your datasets for fine-tuning:

<b>
  <u>1. Data Quality:</u>
</b>

Before fine-tuning models, ensure your data is of high quality. Clean data by removing outliers, duplicates, handle missing values appropriately and make sure you remove any irrelevant data.
It is best to use a tool for this such as [NomicAI](https://home.nomic.ai/) in order to see a map of your data and easily remove data that is not relevant to your use case.

It is important to keep a diverse dataset and make sure your dataset is balanced. For example, make sure there are a similar number of positive outcomes than there
are negative outcomes as well as a similar number of female to male records etc.

<b>
  <u>2. Use Case Understanding: </u>
</b>

The approach for fine-tuning should align with the specifics of your use case. Different problems require varying degrees of precision, recall, speed, etc.
For example, if the model is being used for a task that requires factual accuracy, then the data used to fine-tune the model should be carefully curated to ensure that it is accurate.

<b>
  <u>3. Data Structure: </u>
</b>

Consider the structure of your data. Sequential data should be formatted differently than non-sequential data. Also, understand which features of your data are most relevant to your model.

While the initial curation of data is a difficult and time-consuming process it is very important for you to continuously monitor your model and the predictions/generates
it gives your users and if those are good or bad. In doing this you can fine-tune your model further and provide better generations and cover more edge cases.
