---
title: "Prompt Templating"
description: "Guide to prompt templating when fine-tuning your language model on Cerebrium."
---

<Note>
  Cerebrium's fine-tuning functionality is in public beta and we are adding
  more functionality each week! If there are any issues or if you have an urgent requirement, please reach
  out to [support](mailto\:support@cerebrium.ai) 
</Note>



## Builtin Templates
By default, we provide two templates for question-and-answering which we have adapted from [Alpaca-Lora format](https://github.com/TianyiPeng/alpaca-lora/tree/main/templates) as our prompt template format. You can use these templates or create your own. For more information on templating see [this page](/cerebrium/fine-tuning/language-models/custom-templates.mdx).

There are two options for the templates, `short` and `long`. The `short` template is a single sentence prompt and the `long` template is a multi-sentence prompt. They are shown below:

### Alpaca template (long)

```yaml
description: "Generic long template for fine-tuning.",
prompt_input: "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n\n### Instruction:\n{instruction}\n\n### Input:\n{input}\n\n### Response:\n",
prompt_no_input: "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n\n### Instruction:\n{instruction}\n\n### Response:\n",
response_split: "### Response:"
```


This would be used by adding the following to your **config.yaml** file:

```yaml
dataset_args:
  prompt_template: "long"
```
### Alpaca template (short)
```yaml
"description": "A shorter template to experiment with.",
"prompt_input": "### Instruction:\n{instruction}\n\n### Input:\n{input}\n\n### Response:\n",
"prompt_no_input": "### Instruction:\n{instruction}\n\n### Response:\n",
"response_split": "### Response:"
```

This would be used by adding the following to your **config.yaml** file:

```yaml
dataset_args:
  prompt_template: "short"
```


## Custom Templates

If you would like to use a custom template, you can do so by adding it to your **config.yaml** file under the `prompt_template` parameter. This should be in the format shown below:

```yaml
dataset_args:
  prompt_template:
    "description": "An optional description for your template",
    "prompt_input": "### Your Instruction:\n{instruction}\n\n### Some context label:\n{input}\n\n### Your response prefix:\n",
    "prompt_no_input":  "### Your Instruction:\n{instruction}\n\n### Your response prefix:\n",
    "response_split": "### Your response prefix:"
```

Where your template contains the following parameters in the template string:
- `{instruction}`: The instruction for the prompt. This will be filled in with the `instruction_column` parameter from your dataset.
- `{input}`: The context for the prompt. This will be filled in with the `context_column` parameter from your dataset.
- `{response}`: The response prefix for the prompt. This will be filled in with the `label_column` parameter from your dataset.

where `instruction_column`, `context_column`, and `label_column` are the parameters you have defined for your dataset in the `dataset_args` section of your config file and correspond to the labels in your dataset.json file.


