---
title: "Training Diffusion Models"
description: "Guide to fine-tuning a diffusion model on Cerebrium"
---

<Note>
  Cerebrium's fine-tuning functionality is in public beta and we are adding
  more functionality each week! If there are any issues or if you have an urgent requirement, please reach
  out to [support](mailto\:support@cerebrium.ai) 
</Note>


This guide will walk you through the process of fine-tuning a diffusion model on Cerebrium.  
While this guide is a high-level overview of the process, you can find more detailed information on the available parameters in the [config](/cerebrium/fine-tuning/diffusion-models/config.mdx) and [dataset](/cerebrium/fine-tuning/diffusion-models/dataset.mdx) sections.

### Starting a job with the CLI

Starting a job on cerebrium requires four things:

- A name for you to use to identify the training job.
- Your API key.
- A config file or json string. See [this section](/cerebrium/fine-tuning/diffusion-models/config.mdx) for more info.
- Your dataset of training images along with the corresponding prompt.

Once you have these, you can start a fine-tuning job using the following command:

```bash
cerebrium train --name <<Name for your training>> --training-tupe "diffuser" --api-key <<Your api key if you haven't logged in>> --config-file <<Path to your config file>>
```

Your `config-file` or `config-string` could alternatively provide all the other parameters.

#### Monitoring

Once your training job has been deployed, you will receive a **job-id\*\***. This can be used to access the job status as well as retrieve the logs.

Checking the training status:

```bash
cerebrium get-training-jobs --last-n <optional parameter to only return the status of the last n runs>
```

Retrieving the training logs can be done with:

```bash
cerebrium get-training-logs <<Your training job ID>>
```

_Note that your training logs will only be availible once the job has started running and will not be stored after it is complete._

_Coming soon: logging your training with **weights and biases** is in the final stages of development._

## Retrieving your training results

Once your training is complete, you can download the training results using:

```bash
cerebrium download-model <<JOB_ID>> <<API_KEY>>
```

This will return a zip file which contains your attention processors and the validation images generated by your model during training.

## Using your Fine-tuned Diffuser

Using your finetuning results is done as follows:

```python

from diffusers import (
    DiffusionPipeline,
    DPMSolverMultistepScheduler,
)
import torch

# Boilerplate loading of model
pipeline = DiffusionPipeline.from_pretrained(
    your_model_name, revision=your_model_revision, torch_dtype=torch.float16
)
pipeline.scheduler = DPMSolverMultistepScheduler.from_config(pipeline.scheduler.config)
pipeline = pipeline.to("cuda")


# LOAD IN YOUR TRAINING RESULTS
# load attention processors from where they are saved in your_results/checkpoints/final/attn_procs/pytorch_lora_weights.bin
pipeline.unet.load_attn_procs(os.path.join(final_output_dir, "attn_procs"))
# And that's all you need to do to load in the finetuned result!
# Now you can run your inference as you would like with the pipeline.


# some inference variables
your_prompt = "Your training prompt that you would like to use here"
num_images =  4 # number of images to generate
your_manual_seed = 42 # a manual seed if you would like repeatable results

# run inference as you normally would
generator = torch.Generator(device="cuda").manual_seed(your_manual_seed)
images = [
    pipeline(your_prompt, num_inference_steps=25, generator=generator).images[0]
    for _ in range(num_images)
]

```


# Example yaml config file
```yaml
#############################################
#  Mandatory Parameters
#############################################

#############################################
#  Optional Parameters
#############################################
# Diffuser params
# revision: "main" # Revision of the diffuser model to use.
# validation_prompt: ~ # an optional validation prompt to use. If ~, will use the training prompt.
# custom_tokenizer: "" # custom tokenizer from AutoTokenizer if required.

# # Dataset params
# train_image_dir:  data/training_class_images/ # Directory of training images.
# prior_class_image_dir: ~ # "data/prior_class_images" # Optional directory of images to use for prior class.
# prior_class_prompt: "Photo of a dog."

# # Training params
# training_args:
#   # General training params
#   num_validation_images: 4 # Number of images to generate in validation.
#   learning_rate: 1e-4
#   num_train_epochs: 10
#   seed: ~
#   resolution: 512 # Resolution to train images at.
#   center_crop: False # Whether to center crop images to resolution.
#   train_batch_size: 2
#   num_prior_class_images: 10
#   prior_class_generation_batch_size: 2
#   prior_loss_weight: 1.0 # Weight of prior loss in the total loss.
#   max_train_steps: ~ # maximum training steps which overrides number of training epochs
#   validation_epochs: 5 # number of epochs before running validation and checkpointing

#   # Training loop params
#   gradient_accumulation_steps: 1
#   learning_rate: 0.0005
#   lr_scheduler: "constant"
#   lr_warmup_steps: 5
#   lr_num_cycles: 1
#   lr_power: 1.0
#   allow_tf32: False
#   max_grad_norm: 1.0
#   mixed_precision: "fp16"
#   prior_generation_precision: ~
#   scale_lr: False
#   use_8bit_adam: True
#   use_xformers: True # Whether to use xformers memory efficient attention or not.
``` 

