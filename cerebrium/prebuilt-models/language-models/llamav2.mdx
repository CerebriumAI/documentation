---
title: "Lama 2"
description: "state-of-the-art open-access large language models released by Meta"
api: "POST https://run.cerebrium.ai/llamav2-7b-chat-webhook/predict"
---

Llama 2 is a family of state-of-the-art open-access large language models released by Meta.The Llama 2 release introduces a family of pretrained and fine-tuned LLMs,
ranging in scale from 7B to 70B parameters (7B, 13B, 70B). The pretrained models come with significant improvements over the Llama 1 models, including being trained
on 40% more tokens, having a much longer context length (4k tokens ðŸ¤¯), and using grouped-query attention for fast inference of the 70B model. To deploy Llama2, you can use the identifier below:

- Llama2 7b: `llamav2-7b`
- Llama2 7b Chat: `llamav2-7b-chat`

Here's an example of how to call the deployed endpoint:

#### Request Parameters

<RequestExample>
```bash Request
  curl --location --request POST 'https://run.cerebrium.ai/llamav2-7b-chat-webhook/predict' \
      --header 'Authorization: <API_KEY>' \
      --header 'Content-Type: application/json' \
      --data-raw '{
        "prompt": "Hey! How are you doing?",
        "max_length": 100,
        "temperature": 0.9
        "top_p": 1.0
        "top_k": 10
        "num_return_sequences": 1
        "repetition_penalty": 2.0
    }'
```
</RequestExample>

<ParamField header="Authorization" type="string" required>
  This is the Cerebrium API key used to authenticate your request. You can get
  it from your Cerebrium dashboard.
</ParamField>
<ParamField body="prompt" type="string" required>
  The prompt you would like llama2 to process.
</ParamField>
<ParamField body="max_length" type="string" optional>
  The maximum number of tokens the model must generate. The default is 100.
</ParamField>
<ParamField body="temperature" type="float" optional>
  The value used to control the randomness in the model's predictions. Higher
  values will result in more random outputs while smaller values make the output
  more deterministic. The default is 1.0.
</ParamField>
<ParamField body="top_p" type="float" optional>
  A parameter also known as nucleus sampling, it's used to control the
  randomness by allowing the model to only consider a minimum number of tokens
  with highest probability that cumulatively add up to the specified top_p. The
  default is 0.0.
</ParamField>
<ParamField body="top_k" type="int" optional>
  The maximum number of highest probability vocab tokens considered for each
  step during the generation of sequences. Reducing top_k will limit the number
  of output possibilities, resulting in more deterministic outputs. The default
  is 50.
</ParamField>
<ParamField body="num_return_sequences" type="int" optional>
  The number of independently computed sequences to return. If set to more than
  1, then the number of sequences generated will be num_return_sequences, each
  one independently computed. The default is 1.
</ParamField>
<ParamField body="repetition_penalty" type="string" optional>
  parameter used in text generation to penalize repeated words or tokens in the
  generated text. This is a number greater than or equal to 1.
</ParamField>

<ResponseExample>

```json Response
{
  "run_id": "dc8f23ab-7237-42dc-b6cf-430abdbba8f7",
  "run_time_ms": 10077.8913497924805,
  "message": "Ran successfully",
  "result": [
    {
      "generated_text": "Hey! How are you doing? I'm doing great! *hugs* So, what's new with you? "
    }
  ]
}
```

</ResponseExample>

#### Response Parameters

<ResponseField name="run_id" type="string" required>
  A unique identifier for the run that you can use to associate prompts with
  webhook endpoints.
</ResponseField>
<ResponseField name="run_time_ms" type="string" required>
  The amount of time in millisecond it took to run your function. This is what
  you will be billed for.
</ResponseField>
<ResponseField name="message" type="string" required>
  Whether of not the response was successful
</ResponseField>
<ResponseField name="result" type="string" required>
  The result generated from Llama2
</ResponseField>
